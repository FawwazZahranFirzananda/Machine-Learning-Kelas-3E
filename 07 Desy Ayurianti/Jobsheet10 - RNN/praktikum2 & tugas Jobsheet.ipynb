{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2cxH8gGxwW9"
      },
      "source": [
        "# **Praktikum 2 Generator Teks dengan RNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKx86bWKBgWT"
      },
      "source": [
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d1TgYzFx4an"
      },
      "source": [
        "1. **Import TensorFlow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S20YFawx2J6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv4PGzEkx7h6"
      },
      "source": [
        "2. **Download Dataset Shakespeare**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeviGRTZx-dm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bdf872c-eade-4ca2-ab40-4e9e31fdca6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvlul4XnyEhb"
      },
      "source": [
        "3. **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9-KWGJ5yG9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1c24c3-7a6b-44c8-b9b1-0b9d0e99440c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvpMhuEIyI8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f89b02a-79ff-47e5-a0c7-99d705fe792e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqmB_JfTyKLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09207c4c-a2b6-4d49-8cc1-bee98fd14c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgEuJqbyyR3R"
      },
      "source": [
        "**Olah Teks**\n",
        "\n",
        "**Vectorize Teks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO09vXWXyW5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564218ea-ec77-4537-c671-e6a2c56e8ebf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im3K4xwqyYZe"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRb_fXu6yumR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f47f7aca-9450-490f-9cab-12eef17a33c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wI4SRG-yx6H"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary = ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-0ZtSMPyzee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7940987-bd64-4ce2-bfcc-964ca1906171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9IzNQU_y5F-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae90b71-d76e-404f-88e8-fe5b6d4258c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxOlbEJGy653"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_NqrpUAzA4N"
      },
      "source": [
        "**Prediksi**\n",
        "\n",
        "**Membuat Traning Set dan Target**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOfy_6G6zCOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19674320-7505-4e1a-e3f3-47f61d25fb4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbr7zs8JzIfr"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6MGz-OGzRju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8331ed70-3437-490f-d706-c4b80369f415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rro2jFNgzTTI"
      },
      "outputs": [],
      "source": [
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGCo7cbRzb-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce52924-29bc-4c0f-ee21-57faf805348d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBPtwHIfzeMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6609e8-0666-45cf-9eb8-658b6722052c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NooUpV6kziCo"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLyQneYCzjzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae28506b-444f-4421-d1b3-8bc3795399b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkRcym23zt5g"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IpE7RIMzveP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31575dc1-b2cc-4873-aac2-bc0795fe78e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RkLWcGiz4QG"
      },
      "source": [
        "**Membuat Batch Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31q4b5ZZz5en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22e8582-5d7a-436b-9cea-b87cce35e457"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_Y1Ml_z8wR"
      },
      "source": [
        "**Buat Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmQrFhuSz7Ep"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0AUygiX0BAm"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4-uLtFR0CmN"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USY5ZENI0FEf"
      },
      "source": [
        "**Uji Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg-mWPyp0GT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28097890-cc97-4a88-98a9-2ac28c81834a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GqWikcB0HpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e581d19-689c-4f92-b2dc-e752b4deedee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe0wqHqH0Kd9"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tifN9xfU0NrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0747d6-9a76-4c40-af19-aab4f77314c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14, 65, 54, 59, 43,  5, 10, 48,  8, 26, 31, 34, 54, 47, 32, 57, 22,\n",
              "       28, 15, 29,  5, 39, 47, 45, 15,  7,  1, 34,  8, 62, 15, 46, 15, 43,\n",
              "       54, 26, 32, 64, 40, 64,  3,  0, 19, 58, 17, 46, 19, 42, 46, 63, 56,\n",
              "       35, 50, 36, 45, 57, 52, 57, 33, 62, 38, 63, 62, 26, 28,  9, 18, 40,\n",
              "       29, 59,  8,  2, 45,  1, 12, 47,  3, 46,  0, 14, 14, 24, 15, 58, 64,\n",
              "        8, 48, 55, 13, 30, 36, 18,  5, 22, 63, 32, 55, 39, 11,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwOFZWc_0Q4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa260893-598b-4a36-97a5-5c7c4e45a00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'.\\nAnd is Aufidius with him? You are they\\nThat made the air unwholesome, when you cast\\nYour stinking '\n",
            "\n",
            "Next Char Predictions:\n",
            " b'Azotd&3i-MRUohSrIOBP&ZhfB,\\nU-wBgBdoMSyay![UNK]FsDgFcgxqVkWfrmrTwYxwMO.EaPt- f\\n;h!g[UNK]AAKBsy-ip?QWE&IxSpZ:[UNK]'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F9TaXBN0TZd"
      },
      "source": [
        "**Train Model**\n",
        "\n",
        "**Tambahan optimizer dan fungsi loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q6WsvE10WsF"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzxRCCfA0YL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b068543d-0b14-4f86-ffeb-56d765a6a294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189996, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my_5A8ek0fWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d079ddbb-a4d2-4611-99e2-6509f41c9461"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.022514"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPrsXcCt0ggn"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw61AWs70hJA"
      },
      "source": [
        "**Konfigurasi Checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BewJO4B0jgQ"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGkQJR8h0oXP"
      },
      "source": [
        "**Lakukan Proses Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUAux8v50pfy"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF47XVPx0qy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b974d2e-d85e-4047-a4de-94297635dadf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 898s 5s/step - loss: 2.7193\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 870s 5s/step - loss: 1.9888\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 870s 5s/step - loss: 1.7108\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 889s 5s/step - loss: 1.5477\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 887s 5s/step - loss: 1.4475\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 886s 5s/step - loss: 1.3789\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 877s 5s/step - loss: 1.3271\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 868s 5s/step - loss: 1.2825\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 858s 5s/step - loss: 1.2417\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 856s 5s/step - loss: 1.2022\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 850s 5s/step - loss: 1.1615\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 847s 5s/step - loss: 1.1204\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 839s 5s/step - loss: 1.0774\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 839s 5s/step - loss: 1.0322\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 831s 5s/step - loss: 0.9818\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 829s 5s/step - loss: 0.9314\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 829s 5s/step - loss: 0.8795\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 828s 5s/step - loss: 0.8277\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 829s 5s/step - loss: 0.7762\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 828s 5s/step - loss: 0.7273\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0tWLgeJ0zf6"
      },
      "source": [
        "**Generate Teks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxLEyfmy02Go"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gJbPKgu032o"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWuUNSdy05Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cba7d2b-b654-4e14-a6ae-61f743b41979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The dangerous dangerous speeching blood;\n",
            "O, but it is nor honour slain by law,\n",
            "Proclaims in-cornations: which shall pardon thee,\n",
            "That our solemnity and fair as made\n",
            "Prief how thou sees' and arm the tyrant's ricers.\n",
            "\n",
            "KING HENRY VI:\n",
            "My Lord of Sobertislamation would have sworn\n",
            "The portering of a friend,\n",
            "Are he will show thy saddle curses.\n",
            "\n",
            "KING RICHARD II:\n",
            "What's the rather that, awaked, I would not\n",
            "do you fetter. Duchison well seen absolution,\n",
            "Do not image bereight to the highbour:--\n",
            "By dishonour'd, would spend both their astemness\n",
            "Is nothing like an hour. Live an Hortantior\n",
            "Loved as you are come thence.\n",
            "\n",
            "Messenger:\n",
            "If it be wish, what hath that bare before I want work.\n",
            "\n",
            "EXETER:\n",
            "The sand and kind of Second Kate still, but would\n",
            "Is law as you, she speaks not so bright,\n",
            "Which to this sentence of my curging tyrann\n",
            "To bod till Rome, and we can carriold\n",
            "The glory of my tale tongue.\n",
            "\n",
            "ESCALUS:\n",
            "Ay, that is more appointed it.\n",
            "\n",
            "MENENIUS:\n",
            "I'll she know this base:\n",
            "The rashly wrong'd him.\n",
            "\n",
            "DUKE VIN \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.995561361312866\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_3uEcup0-U1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d397218c-aa17-434c-bc7d-439d7b2d52c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWhence, I pray now, you shall grieve you now.\\n\\nLOHD SIRF DIO:\\nWho often forbid myself!\\n\\nCLAUET:\\nO he dischare that name, forbears!\\nWhat man we must for you, sir abide is long.\\nThe father is well approved all must, why,\\nthen this is mine obsio, here entreat me.\\n\\nQUEEN ELIZABETH:\\nHow fares I keep of crossfitt of.\\n\\nLUCENTIO:\\nO present time, friends, mine is yours.\\nYou promised me, that from the Lord Hastings\\nOf Rome woe in thy tatcher'd rootest and Lord Stand,\\nAnd full of venity boach with her excuse.\\n\\nRICHMOND:\\nFlower with me, thou mightst kill thy eyes\\nJegrens from the warding of my money, never march\\nTo win her, let us by secret heart\\nHath not the time from the holy crace;\\nBut then in persuade he Should be;\\nAnd therein well commanded to his head,\\nNor executel to a man were cheeked from thee.\\nI'll undertake thy fortune or\\na young swift sims too reginent to thee,\\nIf we have still still soundantly face, and\\nmeant to scold. Then valiant means, newly falling.\\n\\nEDWARD:\\nWhere is the dainty o\"\n",
            " b\"ROMEO:\\nThe commonwealty son. Then say it must be alwaysted\\nWhich he shall give him a way.\\n\\nCORIOLANUS:\\nNo;\\n'Twas obstance be spoke froir of revenge\\nOut of thy sad to this, and he wept: and there,\\nAnd, towing our pervorted screpulstood for the ground;\\nOr with all spirits now of mine own life,\\nThe drowshy books of live by out of sighs;\\nAnd not not honour and proceedings\\nOf, plainess lengthen'd hours on her,\\nAufidius, threateness which I slew in prison,\\nNature that thou shalt not halter'd in thy tears.\\n\\nCLIFFORD:\\nAy, when us take I drink atto the placest he\\nunderstand in your bed, ere I can slander\\nWhere to be ruled, upon her false vow\\nNever to turn your varlets high?\\nThe father, to thy wars thereof: I see\\nShe is some seven thousand twenty shoes,\\nThat long usurp'd in places.\\n\\nROMEO:\\nCall forth Bagais and ambition stocks:\\nLet me grey be shall dine.\\n\\nDUKE VINCENTIO:\\nO lie choosed unbrate, and seeing what you pluck\\nThy memitys shall dingeroom from\\nIn my true manifested lusty which speed;\\nAnd with \"\n",
            " b\"ROMEO:\\nThe son and I know the crown he of this slander.\\n\\nMARIANA:\\nO you dispatch'd,\\nWarwick shall die to-morrow to no manks of injury,\\nTarest fear to last in some confidence with\\nI have thy mistress, hath thy words show'd up;\\nAnd therefore, be it known to thee,\\nWhen they shall part your lord and the otherase,\\nOr I my scandal to the nappiness.\\nThy subjects slain by your own trunk Heart's\\nease with neighbours. But is heard of thee\\n\\nVINCENTIO:\\nI have possible an apt--at i' the carken flock than swore,\\nAnd I desire short with thee he would.\\n\\nLOKE SICINIUS:\\nWhat will you are over?\\n\\nPOLIXENES:\\nYou are then;\\nOne more, my fill. When art good on thy sound'?\\nAnd why look you, the other that I dear\\n'Tain the nights betwixt us whether defear,\\nAs if a trebhe uptreven the slip.\\n\\nLADY ANNE:\\nThese both me how you can, my gracious lady a\\nOrm'dam doth thee a beggar for't:\\nAlways be it all, domorr with ourself:\\nNow are they believe this sentence, my woman's temple\\nHished by heir in such a fashion ever\\nSpeak to\"\n",
            " b\"ROMEO:\\nThe sun thus will not rule my braging bark:\\nBut I'll not rave him.\\n\\nDUKE VINCENTIO:\\nWhat is your will?\\n\\nCAMILLO:\\nI saw sweet good night.\\n\\nPRINCE EDWARD:\\nSir John with Clifford, and SurPRiCiTiR:\\nCome, boy.\\n\\nBRUTUS:\\nYour exile, is not my will.\\n\\nWARWICK:\\nTake thou the old man e'er successfuiled pawn the\\nseven to the safermation of thy sons,\\nAnd never seen velvet. Dare not this,\\nI crave but love it by: and that thou weet'st, to speak.\\nMethinks his name rise, what thou her curgins\\nDo execute thy tooth goodly tyranny\\nTo bad submission will our flesh.\\nWhat my old words insolence that I am.\\n\\nMENENIUS:\\nAy, good nine.\\n\\nROMEO:\\nShe may, I do, and I arraid. There lies the king.\\n\\nNORTHUMBERLAND:\\nHow fares our loving mutined that is good in such a gold\\nTo speak of calking 'pardon' brackled and substantiases\\nWith that sparingly purpetual charge effects;\\nMuster to my person men shut.\\nBut what satisfaction comes here be well?\\n\\nRICHARD:\\nWhen I have made thee stealful.\\n\\nyou can, my soul is out and peace.\"\n",
            " b\"ROMEO:\\nThou speak'st a gentleman that she stoops to steal.\\n\\nDUKE OF AUMERLE:\\nMy lady, and He stand worn of France.\\n\\nQUEEN MARGARET:\\nHow canst thou shalt be fails, good grown fellow.\\n\\nISABELLA:\\nTo tras the waters of Hurberland,\\nWould seem busied and guilty of my course,\\nAnd still the oath hath made a monster, nought,\\nIs paritiled armed so soon as desperate,\\nWill our revenge on Richmond's winds,\\nNow poison any four embassanges.\\n\\nVOLUMNIA:\\nOf such noble masters of a doom inquires\\nHere reply, the more my willingly.\\n\\nCAMILLO:\\nSir, I pray thee:\\nThou liest, hour with this, mightrys, many and groans,\\nThat blood is hath corrscared to steal.\\n\\nDUKE OF AUMERLE:\\nUnsulter, understand to command.\\n\\nPOMPEY:\\nYour dream, I shall stand alone.\\nGood night! or I would put your knees you have,\\nMy cousin's pardon.--I, now it please:\\nI would assure he were displeasure so husband?\\n\\nRICHMOND:\\nThen lay hand on woman's matching,\\nYou do prompt such flowing about shrew!\\nArm, arm there!\\n\\nESCALUS:\\nAy. Mark what a warr there.\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2745256423950195\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYRN-o2-1AEo"
      },
      "source": [
        "**Ekspor Model Generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vYNO_Ei1BEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ffe26b-3e00-4710-925c-367d4d144827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7dff445833a0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cwk3e_91DIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3ee22a-ea8e-4de7-96d4-e3ac5b65ec35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The very tricks of that the moPthook of art?\n",
            "\n",
            "All:\n",
            "Clartis, welcome!\n",
            "\n",
            "AUFIDIUS:\n",
            "Where is he?\n",
            "\n",
            "CLIFF\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh-XhPiW1HIf"
      },
      "source": [
        "# **Tugas Pertemuan 10**\n",
        "\n",
        "Gunakan tf.GradientTape untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide.\n",
        "Prosedurnya adalah \"\n",
        "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbwYt09b1SIV"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "     @tf.function\n",
        "     def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uljHGu651T_d"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rft-8rt21VX3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIhJPSwA1XHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baaa92d9-5920-42fe-8689-221c1a1d4c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 861s 5s/step - loss: 2.7403\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dff355dea70>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3Zi13JU1YpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec0c538-d11c-4f86-adc4-e5f47ea551c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2026\n",
            "Epoch 1 Batch 50 Loss 2.0802\n",
            "Epoch 1 Batch 100 Loss 1.9874\n",
            "Epoch 1 Batch 150 Loss 1.8771\n",
            "\n",
            "Epoch 1 Loss: 2.0040\n",
            "Time taken for 1 epoch 861.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8803\n",
            "Epoch 2 Batch 50 Loss 1.7759\n",
            "Epoch 2 Batch 100 Loss 1.7446\n",
            "Epoch 2 Batch 150 Loss 1.6926\n",
            "\n",
            "Epoch 2 Loss: 1.7200\n",
            "Time taken for 1 epoch 840.04 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6129\n",
            "Epoch 3 Batch 50 Loss 1.5191\n",
            "Epoch 3 Batch 100 Loss 1.5911\n",
            "Epoch 3 Batch 150 Loss 1.5313\n",
            "\n",
            "Epoch 3 Loss: 1.5578\n",
            "Time taken for 1 epoch 861.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5391\n",
            "Epoch 4 Batch 50 Loss 1.4892\n",
            "Epoch 4 Batch 100 Loss 1.4622\n",
            "Epoch 4 Batch 150 Loss 1.4338\n",
            "\n",
            "Epoch 4 Loss: 1.4583\n",
            "Time taken for 1 epoch 861.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4042\n",
            "Epoch 5 Batch 50 Loss 1.3967\n",
            "Epoch 5 Batch 100 Loss 1.3912\n",
            "Epoch 5 Batch 150 Loss 1.3799\n",
            "\n",
            "Epoch 5 Loss: 1.3894\n",
            "Time taken for 1 epoch 862.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3311\n",
            "Epoch 6 Batch 50 Loss 1.3322\n",
            "Epoch 6 Batch 100 Loss 1.3072\n",
            "Epoch 6 Batch 150 Loss 1.3250\n",
            "\n",
            "Epoch 6 Loss: 1.3360\n",
            "Time taken for 1 epoch 835.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2868\n",
            "Epoch 7 Batch 50 Loss 1.2549\n",
            "Epoch 7 Batch 100 Loss 1.2723\n",
            "Epoch 7 Batch 150 Loss 1.2987\n",
            "\n",
            "Epoch 7 Loss: 1.2913\n",
            "Time taken for 1 epoch 861.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2443\n",
            "Epoch 8 Batch 50 Loss 1.2647\n",
            "Epoch 8 Batch 100 Loss 1.2385\n",
            "Epoch 8 Batch 150 Loss 1.2734\n",
            "\n",
            "Epoch 8 Loss: 1.2506\n",
            "Time taken for 1 epoch 835.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1805\n",
            "Epoch 9 Batch 50 Loss 1.2002\n",
            "Epoch 9 Batch 100 Loss 1.2271\n",
            "Epoch 9 Batch 150 Loss 1.2344\n",
            "\n",
            "Epoch 9 Loss: 1.2108\n",
            "Time taken for 1 epoch 861.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1687\n",
            "Epoch 10 Batch 50 Loss 1.1702\n",
            "Epoch 10 Batch 100 Loss 1.1811\n",
            "Epoch 10 Batch 150 Loss 1.1484\n",
            "\n",
            "Epoch 10 Loss: 1.1719\n",
            "Time taken for 1 epoch 833.92 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?"
      ],
      "metadata": {
        "id": "K-QEttpOnywF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Potongan Kode Praktikum 2 (Penggunaan Fungsionalitas Keras)**\n",
        "\n",
        "1. Penggunaan Fungsionalitas Keras Terintegrasi: Potongan kode pada praktikum 2 menggunakan lebih banyak fungsionalitas bawaan dari TensorFlow/Keras.\n",
        "2. Penggunaan Layer Keras: Model didefinisikan dengan menggunakan lapisan-lapisan Keras seperti Embedding, GRU, dan Dense. Ini membuat penggunaan model lebih mudah karena Keras mengelola sebagian besar operasi.\n",
        "3. Training dengan Fungsi fit(): Model dilatih menggunakan metode fit() yang lebih terintegrasi dan menangani hampir semua aspek pelatihan.\n",
        "4. Penggunaan Callbacks: Potongan kode ini juga menggunakan callbacks dalam proses pelatihan, khususnya ModelCheckpoint untuk menyimpan bobot model setelah setiap epoch.\n",
        "\n",
        "**Potongan Kode Tugas (Model Custom)**\n",
        "\n",
        "1. Model Custom: Potongan kode di Tugas membahas penggunaan model yang dibuat secara manual dengan pendekatan yang lebih langsung.\n",
        "2. Pembuatan Model dan Pelatihan Langsung: Di sini, Anda membuat kelas CustomTraining yang mewarisi MyModel. Anda kemudian mendefinisikan fungsi train_step secara eksplisit untuk melakukan langkah pelatihan (training step) menggunakan tf.GradientTape dan mengimplementasikan bagian pelatihan secara manual.\n",
        "3. Loop Pelatihan Manual: Anda memiliki loop pelatihan eksplisit yang mengatur pelatihan dalam epoch dan batch secara terperinci.\n",
        "4. Penyimpanan Berdasarkan Epoch: Ada perintah untuk menyimpan bobot model setelah selesainya setiap epoch tertentu.\n",
        "\n",
        "**Perbandingan**\n",
        "\n",
        "1. Level Abstraksi: Potongan kode di Tugas memberikan kontrol yang lebih besar dan membutuhkan lebih banyak penulisan kode eksplisit untuk melatih model.\n",
        "2. Kustomisasi dan Kemudahan Penggunaan: Kode di Tugas memberikan tingkat kustomisasi yang lebih besar dan memungkinkan pengguna untuk mengontrol setiap langkah secara spesifik. Di sisi lain, potongan kode Praktikum 2 menggunakan fungsionalitas bawaan TensorFlow/Keras yang memberikan kemudahan penggunaan dengan menyediakan lebih banyak fitur terintegrasi.\n",
        "3. Pemahaman yang Lebih Mendalam: Kode di Tugas cenderung memberikan pemahaman yang lebih dalam bagaimana setiap langkah dalam pelatihan bekerja, sementara kode Praktikum 2 lebih terfokus pada pemakaian yang lebih mudah dan efisien."
      ],
      "metadata": {
        "id": "XjNPXz-Zng1G"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}