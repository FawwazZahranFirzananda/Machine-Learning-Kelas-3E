{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2cxH8gGxwW9"
      },
      "source": [
        "# **Praktikum 2 Generator Teks dengan RNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKx86bWKBgWT"
      },
      "source": [
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d1TgYzFx4an"
      },
      "source": [
        "1. **Import TensorFlow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S20YFawx2J6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf #latih jaringan saraf\n",
        "import numpy as np #operasi numerik\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv4PGzEkx7h6"
      },
      "source": [
        "2. **Download Dataset Shakespeare**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeviGRTZx-dm",
        "outputId": "5bdf872c-eade-4ca2-ab40-4e9e31fdca6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#download dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvlul4XnyEhb"
      },
      "source": [
        "3. **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9-KWGJ5yG9J",
        "outputId": "2f1c24c3-7a6b-44c8-b9b1-0b9d0e99440c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')#load dataset\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')#cetak panjang teks dalam karakter "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvpMhuEIyI8u",
        "outputId": "0f89b02a-79ff-47e5-a0c7-99d705fe792e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])#cetak teks pada 250 karakter pertama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqmB_JfTyKLt",
        "outputId": "09207c4c-a2b6-4d49-8cc1-bee98fd14c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "#set(text) untuk buat himpunan karakter unik \n",
        "#sorted untuk mengurutkan himpunan karakter unik \n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')#cetak jumlah karakter unik "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgEuJqbyyR3R"
      },
      "source": [
        "**Olah Teks**\n",
        "\n",
        "**Vectorize Teks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO09vXWXyW5P",
        "outputId": "564218ea-ec77-4537-c671-e6a2c56e8ebf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "#unicode_split untuk memishakan setiap teks jadi karakter individual \n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars #berisi tensor dg karakter individual dari teks contoh "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im3K4xwqyYZe"
      },
      "outputs": [],
      "source": [
        "#membuat indeks karakter ke dlm id-nya\n",
        "\n",
        "#StringLookup untuk ubah karakter menjadi IDnya \n",
        "#vocabulary=list(vocab) daftar karakter unik yang telah dibuat sebelumnya \n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRb_fXu6yumR",
        "outputId": "f47f7aca-9450-490f-9cab-12eef17a33c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#konversi karakter 'chars' menjadi ID-nya\n",
        "ids = ids_from_chars(chars)#konversi 'chars' menajdi ID-nya\n",
        "ids #hasil konversi disimpan dalam variabel ini "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wI4SRG-yx6H"
      },
      "outputs": [],
      "source": [
        "#Buat objek 'chars_from_ids' untuk mengubah kembali IO menjadi karakter aslinya\n",
        "#stringlookup untuk buat objek \n",
        "#Parameter vocabulary diatur ke ids_from_chars.get_vocabulary(). \n",
        "# Fungsi get_vocabulary() digunakan untuk mengambil kamus karakter unik yang telah digunakan sebelumnya\n",
        "#invert diset True untuk mengindikasikan konversi yang dilkaukan adalah dari ID ke karakter aslinya \n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary = ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-0ZtSMPyzee",
        "outputId": "f7940987-bd64-4ce2-bfcc-964ca1906171"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Konversi kembali ID menjad karakter aslinya\n",
        "chars = chars_from_ids(ids)#chars_from_ids untuk konversi ID dalam variabel ids, ids itu tensor yang isinya ID yang telah dihasilkan sebelumnya\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9IzNQU_y5F-",
        "outputId": "7ae90b71-d76e-404f-88e8-fe5b6d4258c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Penggabungan elemen dalam tensor chars \n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxOlbEJGy653"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):#fungsi untuk konversi bilangan atau indeks ke karakter yg sesuai\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)#menggabungkan karakter menjadi satu string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_NqrpUAzA4N"
      },
      "source": [
        "**Prediksi**\n",
        "\n",
        "**Membuat Traning Set dan Target**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOfy_6G6zCOU",
        "outputId": "19674320-7505-4e1a-e3f3-47f61d25fb4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ids_from_chars() fungsi untul ambil tensor yang berisi kode unicode\n",
        "# tf.strings.unicode_split(text, 'UTF-8') untuk bagi string 'text' jadi unicode dg UTF-8\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbr7zs8JzIfr"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)#buat dataset dari all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6MGz-OGzRju",
        "outputId": "8331ed70-3437-490f-d706-c4b80369f415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):#loop untuk ambil 10 elemen pertama dataset ids_dataset\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))#ambil elemen ids, terus konversi jadi karakter pake fungsi chars_from_ids(ids), setelah itu konversi karakter lagi jadi string pake numpy().decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rro2jFNgzTTI"
      },
      "outputs": [],
      "source": [
        "seq_length = 100#untuk jumlah sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGCo7cbRzb-1",
        "outputId": "0ce52924-29bc-4c0f-ee21-57faf805348d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "#Gabungi urutan dari ID jadi urutan yang lebih panjang \n",
        "\n",
        "#gabungi ID jadi sekuens dengan panjang 101,\n",
        "#drop_remainder=True mengindikasikan bahwa jika panjang ID tidak habis dibagi oleh seq_length + 1, maka elemen yang tersisa akan dihapus\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)#kelola data sekuens\n",
        "for seq in sequences.take(1):#ambil satu sekuenes /batch \n",
        "  print(chars_from_ids(seq))#cetak karakter "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBPtwHIfzeMz",
        "outputId": "3b6609e8-0666-45cf-9eb8-658b6722052c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):#ambil 5 sekuens pertama \n",
        "    print(text_from_ids(seq).numpy())#cetak teks yang mana diambil sekuens ID dulu terus dikeonversi jadi teks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NooUpV6kziCo"
      },
      "outputs": [],
      "source": [
        "#bagi sekuens \n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]#ambil semua elemen kecuali elemen terakhir dai sekuens\n",
        "    target_text = sequence[1:]#ambil semua elmeen kecuali elemen pertama dari sekuens\n",
        "    return input_text, target_text#mengembaikan nilai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLyQneYCzjzq",
        "outputId": "ae28506b-444f-4421-d1b3-8bc3795399b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))#bagi sequence jadi 2, input (Tensorflo) dan target (ensorflow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkRcym23zt5g"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)#mapping sekuens dalam dataset ke pasangan input dan target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IpE7RIMzveP",
        "outputId": "31575dc1-b2cc-4873-aac2-bc0795fe78e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "#lopp ambil 1 pasangan input dan target\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())#cetak label input, text_from_ids(input_example) mengonversi input_example, yang berisi sekuens ID, kembali menjadi teks, dan .numpy() mengubah hasilnya menjadi string yang dapat dicetak.\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())#cetak label target, text_from_ids(target_example) mengonversi target_example, yang berisi sekuens ID target, kembali menjadi teks, dan .numpy() mengubah hasilnya menjadi string yang dapat dicetak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RkLWcGiz4QG"
      },
      "source": [
        "**Membuat Batch Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31q4b5ZZz5en",
        "outputId": "a22e8582-5d7a-436b-9cea-b87cce35e457"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64 #menentukan ukuran batch  untuk melatih model \n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000 #menentukan ukuran buffer diapke buat pengacakan pada dataset\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)#acak elemen \n",
        "    .batch(BATCH_SIZE, drop_remainder=True)#mengelompokkan elemen dataset menjadi batch\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))#mengoptimalkan proses memuat data\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_Y1Ml_z8wR"
      },
      "source": [
        "**Buat Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmQrFhuSz7Ep"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())#untuk tau brp banyak karakter dalam vocab yg digunakan dalam model \n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256#jumlah embedding untuk represntasi vektor karakter\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024#jumlah unit dalam RNN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0AUygiX0BAm"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):#kelas dimana MyModel adalah model yang dapat digunakan dalam TensorFlow.\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):#inisialisasi objek \n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)#konversi ID karakter menjadi vektor embedding\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)#lapisan RNN (Recurrent Neural Network) yang digunakan untuk memahami urutan data\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)#untuk memprediksi karakter selanjutnya dalam urutan.\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):#mendefinisikan alur perhitungan (forward pass) model.\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)#sekuens ID karakter diubah menjadi representasi vektor karakter dengan menggunakan lapisan embedding\n",
        "    if states is None:#kondisi yang memeriksa apakah status awal RNN (states) telah diberikan. Jika belum, maka status awal diperoleh dari lapisan RNN.\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)#sekuens karakter masukan diproses melalui lapisan RNN. Hasilnya adalah urutan sekuens karakter yang diprediksi serta status terakhir lapisan RNN.\n",
        "    x = self.dense(x, training=training)#hasil dari lapisan RNN digunakan untuk memprediksi karakter selanjutnya \n",
        "\n",
        "    if return_state:\n",
        "      return x, states#mengembalikan urutan karakter yang diprediksi serta status terakhir RNN.\n",
        "    else:\n",
        "      return x#hanya akan mengembalikan urutan karakter yang diprediksi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4-uLtFR0CmN"
      },
      "outputs": [],
      "source": [
        "model = MyModel(#inisialisasi model \n",
        "    vocab_size=vocab_size,#menentukan seberapa banyak karakter yang ada dalam vokabulari yang digunakan oleh model\n",
        "    embedding_dim=embedding_dim,#untuk mengontrol kompleksitas representasi vektor karakter dalam model.\n",
        "    rnn_units=rnn_units)# jumlah unit dalam lapisan RNN untuk mengatur seberapa kuat lapisan RNN dalam memahami konteks urutan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USY5ZENI0FEf"
      },
      "source": [
        "**Uji Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg-mWPyp0GT6",
        "outputId": "28097890-cc97-4a88-98a9-2ac28c81834a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):#oop yang mengambil satu batch pertama dari dataset.\n",
        "    example_batch_predictions = model(input_example_batch)#membuat prediksi dengan memberikan input_example_batch sebagai input. Hasilnya adalah example_batch_predictions, yang akan berisi prediksi model\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")#mencetak bentuk (shape) dari example_batch_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GqWikcB0HpT",
        "outputId": "8e581d19-689c-4f92-b2dc-e752b4deedee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()#cetak ringkasan (summary) dari model TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe0wqHqH0Kd9"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)#untuk prediksi model untuk satu contoh dalam batch\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()#untuk menghapus dimensi tambahan dan menghasilkan tensor satu dimensi yang berisi indeks karakter yang diambil secara acak dan mengambil data kemudian dikonversi jadi array numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tifN9xfU0NrW",
        "outputId": "3d0747d6-9a76-4c40-af19-aab4f77314c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([14, 65, 54, 59, 43,  5, 10, 48,  8, 26, 31, 34, 54, 47, 32, 57, 22,\n",
              "       28, 15, 29,  5, 39, 47, 45, 15,  7,  1, 34,  8, 62, 15, 46, 15, 43,\n",
              "       54, 26, 32, 64, 40, 64,  3,  0, 19, 58, 17, 46, 19, 42, 46, 63, 56,\n",
              "       35, 50, 36, 45, 57, 52, 57, 33, 62, 38, 63, 62, 26, 28,  9, 18, 40,\n",
              "       29, 59,  8,  2, 45,  1, 12, 47,  3, 46,  0, 14, 14, 24, 15, 58, 64,\n",
              "        8, 48, 55, 13, 30, 36, 18,  5, 22, 63, 32, 55, 39, 11,  0])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices# indeks karakter yang diambil secara acak dari prediksi model untuk satu contoh dalam batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwOFZWc_0Q4R",
        "outputId": "fa260893-598b-4a36-97a5-5c7c4e45a00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'.\\nAnd is Aufidius with him? You are they\\nThat made the air unwholesome, when you cast\\nYour stinking '\n",
            "\n",
            "Next Char Predictions:\n",
            " b'Azotd&3i-MRUohSrIOBP&ZhfB,\\nU-wBgBdoMSyay![UNK]FsDgFcgxqVkWfrmrTwYxwMO.EaPt- f\\n;h!g[UNK]AAKBsy-ip?QWE&IxSpZ:[UNK]'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())#cetak teks masukan dari contoh pertama dalam batch(yang ID karakternya sudah dikonversi jadi teks kemudian diubah ke bentuk string)\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())#cetak prediksi karakter selanjutnya yang dihasilkan oleh model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F9TaXBN0TZd"
      },
      "source": [
        "**Train Model**\n",
        "\n",
        "**Tambahan optimizer dan fungsi loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q6WsvE10WsF"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)#ntuk pengenalan pola pada data urutan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzxRCCfA0YL2",
        "outputId": "b068543d-0b14-4f86-ffeb-56d765a6a294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189996, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "#hitung kerugian (loss) rata-rata pada satu batch\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)#hitung kerugian rata-rata\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")#cetak bentuk dari example_batch_predictions. \n",
        "print(\"Mean loss:        \", example_batch_mean_loss)#cetak nilai kerugian rata-rata "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my_5A8ek0fWu",
        "outputId": "d079ddbb-a4d2-4611-99e2-6509f41c9461"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66.022514"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()#hitung nilai eksponensial dari nilai kerugian rata-rata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPrsXcCt0ggn"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)#compile model sebelum pelatihan model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw61AWs70hJA"
      },
      "source": [
        "**Konfigurasi Checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BewJO4B0jgQ"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints' #direktori di mana checkpoint akan disimpan.\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")#checkpoint akan memiliki nama yang mencerminkan epoch pelatihan.\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(#fungsi untuk untuk menyimpan checkpoint.\n",
        "    filepath=checkpoint_prefix,#untuk tempat penyimpanan file checkpoint \n",
        "    save_weights_only=True)#untuk menunjukkan bahwa hanya bobot model yang akan disimpan dalam checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGkQJR8h0oXP"
      },
      "source": [
        "**Lakukan Proses Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUAux8v50pfy"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20 #jumlah epoch iterasi pelatihan "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF47XVPx0qy1",
        "outputId": "8b974d2e-d85e-4047-a4de-94297635dadf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 898s 5s/step - loss: 2.7193\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 870s 5s/step - loss: 1.9888\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 870s 5s/step - loss: 1.7108\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 889s 5s/step - loss: 1.5477\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 887s 5s/step - loss: 1.4475\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 886s 5s/step - loss: 1.3789\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 877s 5s/step - loss: 1.3271\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 868s 5s/step - loss: 1.2825\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 858s 5s/step - loss: 1.2417\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 856s 5s/step - loss: 1.2022\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 850s 5s/step - loss: 1.1615\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 847s 5s/step - loss: 1.1204\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 839s 5s/step - loss: 1.0774\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 839s 5s/step - loss: 1.0322\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 831s 5s/step - loss: 0.9818\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 829s 5s/step - loss: 0.9314\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 829s 5s/step - loss: 0.8795\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 828s 5s/step - loss: 0.8277\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 829s 5s/step - loss: 0.7762\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 828s 5s/step - loss: 0.7273\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])#latih model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0tWLgeJ0zf6"
      },
      "source": [
        "**Generate Teks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxLEyfmy02Go"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):#definisi objek kelas yg merupakan model kecil yang digunakan untuk menghasilkan teks dalam satu langkah\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):#metode konstruktor (constructor) yang digunakan untuk menginisialisasi objek \n",
        "    super().__init__()\n",
        "    #Membuat atribut-atribut seperti temperature, model, chars_from_ids, dan ids_from_chars \n",
        "    # untuk menyimpan parameter-parameter yang diberikan saat inisialisasi.\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]#mengonversi karakter \"[UNK]\" menjadi ID karakter\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),#membuat daftar nilai yang berisi -infiniti (nilai negatif tak hingga) sebanyak jumlah ID karakter yang sesuai dengan karakter \"[UNK]\"\n",
        "        indices=skip_ids,#memberikan nilai -infiniti pada ID karakter \"[UNK]\".\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])#definisikan bentuk (shape) masker sebagai panjang dari vokabulari karakter \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)#mengonversi masker dari bentuk sparse tensor ke bentuk dense tensor dengan menggunakan \n",
        "\n",
        "  @tf.function#decorator yang digunakan untuk mengubah metode generate_one_step menjadi fungsi TensorFlow yang dapat dijalankan secara efisien.\n",
        "  def generate_one_step(self, inputs, states=None):#untuk menghasilkan teks dalam satu langkah\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')#mengonversi string masukan (inputs) menjadi urutan token karakter.\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()#mengonversi token karakter menjadi token ID karakter\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)#menjalankan model dengan masukan input_ids untuk memprediksi karakter selanjutnya\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]#hanya hasil prediksi karakter selanjutnya yang diambil, yaitu karakter terakhir dalam urutan (-1).\n",
        "    predicted_logits = predicted_logits/self.temperature#untuk mengendalikan tingkat keacakan dalam hasil prediksi\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask#menghindari karakter \"[UNK]\" (unknown) dari yang dihasilkan oleh model\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)#mengambil sampel dari logit hasil prediksi untuk menghasilkan token ID karakter selanjutnya\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)#menghapus dimensi tambahan dan menghasilkan tensor satu dimensi yang berisi token ID karakter selanjutnya.\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)#mengonversi token ID karakter menjadi karakter \n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states#mengembalikan karakter-karakter yang dihasilkan (predicted_chars) dan status model (states) sebagai hasil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gJbPKgu032o"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)#membuat objek one_step_model, Anda dapat menggunakan metode generate_one_step untuk menghasilkan karakter selanjutnya dalam urutan berdasarkan masukan yang diberikan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWuUNSdy05Lf",
        "outputId": "0cba7d2b-b654-4e14-a6ae-61f743b41979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "The dangerous dangerous speeching blood;\n",
            "O, but it is nor honour slain by law,\n",
            "Proclaims in-cornations: which shall pardon thee,\n",
            "That our solemnity and fair as made\n",
            "Prief how thou sees' and arm the tyrant's ricers.\n",
            "\n",
            "KING HENRY VI:\n",
            "My Lord of Sobertislamation would have sworn\n",
            "The portering of a friend,\n",
            "Are he will show thy saddle curses.\n",
            "\n",
            "KING RICHARD II:\n",
            "What's the rather that, awaked, I would not\n",
            "do you fetter. Duchison well seen absolution,\n",
            "Do not image bereight to the highbour:--\n",
            "By dishonour'd, would spend both their astemness\n",
            "Is nothing like an hour. Live an Hortantior\n",
            "Loved as you are come thence.\n",
            "\n",
            "Messenger:\n",
            "If it be wish, what hath that bare before I want work.\n",
            "\n",
            "EXETER:\n",
            "The sand and kind of Second Kate still, but would\n",
            "Is law as you, she speaks not so bright,\n",
            "Which to this sentence of my curging tyrann\n",
            "To bod till Rome, and we can carriold\n",
            "The glory of my tale tongue.\n",
            "\n",
            "ESCALUS:\n",
            "Ay, that is more appointed it.\n",
            "\n",
            "MENENIUS:\n",
            "I'll she know this base:\n",
            "The rashly wrong'd him.\n",
            "\n",
            "DUKE VIN \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.995561361312866\n"
          ]
        }
      ],
      "source": [
        "start = time.time()#untuk mengukur berapa lama waktu yang diperlukan untuk menghasilkan teks\n",
        "states = None#status model diatur sebagai None yg digunakan untuk melacak status model saat menghasilkan teks.\n",
        "next_char = tf.constant(['ROMEO:'])# karakter awal yang digunakan sebagai masukan\n",
        "result = [next_char]#daftar (list) yang digunakan untuk menyimpan karakter-karakter yang dihasilkan oleh model\n",
        "\n",
        "for n in range(1000):#ntuk menghasilkan teks sebanyak 1000 karakter\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)#memanggil metode generate_one_step dengan karakter next_char sebagai masukan dan status states saat ini.\n",
        "  result.append(next_char)#Karakter selanjutnya (next_char) ditambahkan ke daftar result.\n",
        "\n",
        "result = tf.strings.join(result)#menggabungkan semua karakter yang telah dihasilkan menjadi satu teks tunggal \n",
        "end = time.time()#untuk menghentikan perhitungan waktu setelah teks dihasilkan.\n",
        "\n",
        "#result[0].numpy() mengambil nilai teks dari tensor TensorFlow,\n",
        "# decode('utf-8') digunakan untuk mengonversi nilai tersebut menjadi string yang bisa dicetak. \n",
        "# dicetak garis pemisah yang berisi 80 karakter garis bawah (_) untuk memisahkan teks dari informasi tambahan.\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)#mencetak teks yang dihasilkan dalam format yang lebih mudah dibaca\n",
        "print('\\nRun time:', end - start)#etak waktu yang diperlukan untuk menghasilkan teks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_3uEcup0-U1",
        "outputId": "d397218c-aa17-434c-bc7d-439d7b2d52c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWhence, I pray now, you shall grieve you now.\\n\\nLOHD SIRF DIO:\\nWho often forbid myself!\\n\\nCLAUET:\\nO he dischare that name, forbears!\\nWhat man we must for you, sir abide is long.\\nThe father is well approved all must, why,\\nthen this is mine obsio, here entreat me.\\n\\nQUEEN ELIZABETH:\\nHow fares I keep of crossfitt of.\\n\\nLUCENTIO:\\nO present time, friends, mine is yours.\\nYou promised me, that from the Lord Hastings\\nOf Rome woe in thy tatcher'd rootest and Lord Stand,\\nAnd full of venity boach with her excuse.\\n\\nRICHMOND:\\nFlower with me, thou mightst kill thy eyes\\nJegrens from the warding of my money, never march\\nTo win her, let us by secret heart\\nHath not the time from the holy crace;\\nBut then in persuade he Should be;\\nAnd therein well commanded to his head,\\nNor executel to a man were cheeked from thee.\\nI'll undertake thy fortune or\\na young swift sims too reginent to thee,\\nIf we have still still soundantly face, and\\nmeant to scold. Then valiant means, newly falling.\\n\\nEDWARD:\\nWhere is the dainty o\"\n",
            " b\"ROMEO:\\nThe commonwealty son. Then say it must be alwaysted\\nWhich he shall give him a way.\\n\\nCORIOLANUS:\\nNo;\\n'Twas obstance be spoke froir of revenge\\nOut of thy sad to this, and he wept: and there,\\nAnd, towing our pervorted screpulstood for the ground;\\nOr with all spirits now of mine own life,\\nThe drowshy books of live by out of sighs;\\nAnd not not honour and proceedings\\nOf, plainess lengthen'd hours on her,\\nAufidius, threateness which I slew in prison,\\nNature that thou shalt not halter'd in thy tears.\\n\\nCLIFFORD:\\nAy, when us take I drink atto the placest he\\nunderstand in your bed, ere I can slander\\nWhere to be ruled, upon her false vow\\nNever to turn your varlets high?\\nThe father, to thy wars thereof: I see\\nShe is some seven thousand twenty shoes,\\nThat long usurp'd in places.\\n\\nROMEO:\\nCall forth Bagais and ambition stocks:\\nLet me grey be shall dine.\\n\\nDUKE VINCENTIO:\\nO lie choosed unbrate, and seeing what you pluck\\nThy memitys shall dingeroom from\\nIn my true manifested lusty which speed;\\nAnd with \"\n",
            " b\"ROMEO:\\nThe son and I know the crown he of this slander.\\n\\nMARIANA:\\nO you dispatch'd,\\nWarwick shall die to-morrow to no manks of injury,\\nTarest fear to last in some confidence with\\nI have thy mistress, hath thy words show'd up;\\nAnd therefore, be it known to thee,\\nWhen they shall part your lord and the otherase,\\nOr I my scandal to the nappiness.\\nThy subjects slain by your own trunk Heart's\\nease with neighbours. But is heard of thee\\n\\nVINCENTIO:\\nI have possible an apt--at i' the carken flock than swore,\\nAnd I desire short with thee he would.\\n\\nLOKE SICINIUS:\\nWhat will you are over?\\n\\nPOLIXENES:\\nYou are then;\\nOne more, my fill. When art good on thy sound'?\\nAnd why look you, the other that I dear\\n'Tain the nights betwixt us whether defear,\\nAs if a trebhe uptreven the slip.\\n\\nLADY ANNE:\\nThese both me how you can, my gracious lady a\\nOrm'dam doth thee a beggar for't:\\nAlways be it all, domorr with ourself:\\nNow are they believe this sentence, my woman's temple\\nHished by heir in such a fashion ever\\nSpeak to\"\n",
            " b\"ROMEO:\\nThe sun thus will not rule my braging bark:\\nBut I'll not rave him.\\n\\nDUKE VINCENTIO:\\nWhat is your will?\\n\\nCAMILLO:\\nI saw sweet good night.\\n\\nPRINCE EDWARD:\\nSir John with Clifford, and SurPRiCiTiR:\\nCome, boy.\\n\\nBRUTUS:\\nYour exile, is not my will.\\n\\nWARWICK:\\nTake thou the old man e'er successfuiled pawn the\\nseven to the safermation of thy sons,\\nAnd never seen velvet. Dare not this,\\nI crave but love it by: and that thou weet'st, to speak.\\nMethinks his name rise, what thou her curgins\\nDo execute thy tooth goodly tyranny\\nTo bad submission will our flesh.\\nWhat my old words insolence that I am.\\n\\nMENENIUS:\\nAy, good nine.\\n\\nROMEO:\\nShe may, I do, and I arraid. There lies the king.\\n\\nNORTHUMBERLAND:\\nHow fares our loving mutined that is good in such a gold\\nTo speak of calking 'pardon' brackled and substantiases\\nWith that sparingly purpetual charge effects;\\nMuster to my person men shut.\\nBut what satisfaction comes here be well?\\n\\nRICHARD:\\nWhen I have made thee stealful.\\n\\nyou can, my soul is out and peace.\"\n",
            " b\"ROMEO:\\nThou speak'st a gentleman that she stoops to steal.\\n\\nDUKE OF AUMERLE:\\nMy lady, and He stand worn of France.\\n\\nQUEEN MARGARET:\\nHow canst thou shalt be fails, good grown fellow.\\n\\nISABELLA:\\nTo tras the waters of Hurberland,\\nWould seem busied and guilty of my course,\\nAnd still the oath hath made a monster, nought,\\nIs paritiled armed so soon as desperate,\\nWill our revenge on Richmond's winds,\\nNow poison any four embassanges.\\n\\nVOLUMNIA:\\nOf such noble masters of a doom inquires\\nHere reply, the more my willingly.\\n\\nCAMILLO:\\nSir, I pray thee:\\nThou liest, hour with this, mightrys, many and groans,\\nThat blood is hath corrscared to steal.\\n\\nDUKE OF AUMERLE:\\nUnsulter, understand to command.\\n\\nPOMPEY:\\nYour dream, I shall stand alone.\\nGood night! or I would put your knees you have,\\nMy cousin's pardon.--I, now it please:\\nI would assure he were displeasure so husband?\\n\\nRICHMOND:\\nThen lay hand on woman's matching,\\nYou do prompt such flowing about shrew!\\nArm, arm there!\\n\\nESCALUS:\\nAy. Mark what a warr there.\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2745256423950195\n"
          ]
        }
      ],
      "source": [
        "start = time.time()#untuk mengukur berapa lama waktu yang diperlukan untuk menghasilkan teks.\n",
        "states = None#mengatur status model awal sebagai None.\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])#arakter awal yang digunakan sebagai masukan\n",
        "result = [next_char]#memulai daftar result dengan karakter-karakter awal yang telah ditentukan \n",
        "\n",
        "for n in range(1000):#menghasilkan teks sebanyak 1000 karakter\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)#menggunakan objek one_step_model untuk memanggil metode generate_one_step dengan karakter next_char sebagai masukan dan status states saat ini.\n",
        "  result.append(next_char)#Karakter selanjutnya (next_char) ditambahkan ke daftar result.\n",
        "\n",
        "result = tf.strings.join(result)#menggabungkan semua karakter yang telah dihasilkan menjadi satu teks tunggal \n",
        "end = time.time()#menghentikan perhitungan waktu setelah teks dihasilkan.\n",
        "print(result, '\\n\\n' + '_'*80)#cetak seluruh teks yang dihasilkan. Garis pemisah yang berisi 80 karakter garis bawah (_) digunakan untuk memisahkan teks dari informasi tambahan.\n",
        "print('\\nRun time:', end - start)#cetak waktu yang diperlukan untuk menghasilkan teks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYRN-o2-1AEo"
      },
      "source": [
        "**Ekspor Model Generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vYNO_Ei1BEl",
        "outputId": "97ffe26b-3e00-4710-925c-367d4d144827"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7dff445833a0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')#menyimpan model one_step_model ke disk dengan nama \"one_step\"\n",
        "one_step_reloaded = tf.saved_model.load('one_step')#memuat kembali model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cwk3e_91DIu",
        "outputId": "4b3ee22a-ea8e-4de7-96d4-e3ac5b65ec35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "The very tricks of that the moPthook of art?\n",
            "\n",
            "All:\n",
            "Clartis, welcome!\n",
            "\n",
            "AUFIDIUS:\n",
            "Where is he?\n",
            "\n",
            "CLIFF\n"
          ]
        }
      ],
      "source": [
        "states = None#mengatur status model awal sebagai None.\n",
        "next_char = tf.constant(['ROMEO:'])#mengatur status model states dan karakter awal next_char sebagai \"ROMEO:\".\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):#enghasilkan teks sebanyak 100 karakter\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)##menggunakan objek one_step_model untuk memanggil metode generate_one_step dengan karakter next_char sebagai masukan dan status states saat ini.\n",
        "  result.append(next_char)#Karakter selanjutnya (next_char) ditambahkan ke daftar result.\n",
        "\n",
        "\n",
        "#tf.strings.join(result): Ini menggabungkan semua karakter yang telah dihasilkan menjadi satu tensor teks tunggal\n",
        "#[0]: mengambil elemen pertama dari tensor teks.\n",
        "#numpy(): Ini mengambil nilai teks dari tensor TensorFlow dan mengonversinya menjadi array NumPy.\n",
        "#decode(\"utf-8\"): Terakhir, ini mengonversi nilai NumPy menjadi string dengan menggunakan dekoder UTF-8.\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh-XhPiW1HIf"
      },
      "source": [
        "# **Tugas Pertemuan 10**\n",
        "\n",
        "Gunakan tf.GradientTape untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide.\n",
        "Prosedurnya adalah \"\n",
        "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbwYt09b1SIV"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):#mendefinisikan kelas CustomTraining yang merupakan turunan dari MyModel\n",
        "     @tf.function\n",
        "     def train_step(self, inputs):#digunakan untuk melatih model dengan satu langkah (step) selama proses pelatihan\n",
        "        inputs, labels = inputs#memisahkan inputs menjadi inputs dan labels. inputs adalah masukan model, dan labels adalah target yang seharusnya diprediksi oleh model.\n",
        "        with tf.GradientTape() as tape:#untuk menghitung gradien (gradient) dari fungsi kerugian terhadap parameter model.\n",
        "            predictions = self(inputs, training=True)#untuk enghasilkan prediksi model\n",
        "            loss = self.loss(labels, predictions)#enghitung nilai kerugian (loss) dengan membandingkan prediksi model dengan label yang seharusnya diprediksi\n",
        "            grads = tape.gradient(loss, model.trainable_variables)#menghitung gradien (gradient) dari kerugian terhadap parameter-parameter yang dapat diubah dalam model (trainable variables)\n",
        "            self.optimizer.apply_gradients(zip(grads, model.trainable_variables))#mengaplikasikan gradien ke parameter-parameter model, sehingga model dapat diperbarui selama pelatihan.\n",
        "\n",
        "            return {'loss': loss}#mengembalikan dictionary yang berisi nilai kerugian (loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uljHGu651T_d"
      },
      "outputs": [],
      "source": [
        "#membuat objek model dengan menggunakan kelas CustomTraining dengan 3 parameter\n",
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),#memberikan informasi berapa banyak karakter yang ada dalam vokabulari model.\n",
        "    embedding_dim=embedding_dim,#dimensi embedding yang digunakan dalam model. \n",
        "    rnn_units=rnn_units)#jumlah unit dalam lapisan RNN model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rft-8rt21VX3"
      },
      "outputs": [],
      "source": [
        "#compile model \n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),#digunakan untuk mengoptimalkan parameter-parameter model selama pelatihan\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))#menghasilkan logit (skor) langsung, dan kerugian akan menghitung probabilitas berdasarkan logit yang dihasilkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIhJPSwA1XHb",
        "outputId": "baaa92d9-5920-42fe-8689-221c1a1d4c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 861s 5s/step - loss: 2.7403\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dff355dea70>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)#elatih model Anda selama satu epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Zi13JU1YpP",
        "outputId": "9ec0c538-d11c-4f86-adc4-e5f47ea551c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2026\n",
            "Epoch 1 Batch 50 Loss 2.0802\n",
            "Epoch 1 Batch 100 Loss 1.9874\n",
            "Epoch 1 Batch 150 Loss 1.8771\n",
            "\n",
            "Epoch 1 Loss: 2.0040\n",
            "Time taken for 1 epoch 861.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8803\n",
            "Epoch 2 Batch 50 Loss 1.7759\n",
            "Epoch 2 Batch 100 Loss 1.7446\n",
            "Epoch 2 Batch 150 Loss 1.6926\n",
            "\n",
            "Epoch 2 Loss: 1.7200\n",
            "Time taken for 1 epoch 840.04 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6129\n",
            "Epoch 3 Batch 50 Loss 1.5191\n",
            "Epoch 3 Batch 100 Loss 1.5911\n",
            "Epoch 3 Batch 150 Loss 1.5313\n",
            "\n",
            "Epoch 3 Loss: 1.5578\n",
            "Time taken for 1 epoch 861.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.5391\n",
            "Epoch 4 Batch 50 Loss 1.4892\n",
            "Epoch 4 Batch 100 Loss 1.4622\n",
            "Epoch 4 Batch 150 Loss 1.4338\n",
            "\n",
            "Epoch 4 Loss: 1.4583\n",
            "Time taken for 1 epoch 861.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4042\n",
            "Epoch 5 Batch 50 Loss 1.3967\n",
            "Epoch 5 Batch 100 Loss 1.3912\n",
            "Epoch 5 Batch 150 Loss 1.3799\n",
            "\n",
            "Epoch 5 Loss: 1.3894\n",
            "Time taken for 1 epoch 862.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3311\n",
            "Epoch 6 Batch 50 Loss 1.3322\n",
            "Epoch 6 Batch 100 Loss 1.3072\n",
            "Epoch 6 Batch 150 Loss 1.3250\n",
            "\n",
            "Epoch 6 Loss: 1.3360\n",
            "Time taken for 1 epoch 835.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2868\n",
            "Epoch 7 Batch 50 Loss 1.2549\n",
            "Epoch 7 Batch 100 Loss 1.2723\n",
            "Epoch 7 Batch 150 Loss 1.2987\n",
            "\n",
            "Epoch 7 Loss: 1.2913\n",
            "Time taken for 1 epoch 861.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2443\n",
            "Epoch 8 Batch 50 Loss 1.2647\n",
            "Epoch 8 Batch 100 Loss 1.2385\n",
            "Epoch 8 Batch 150 Loss 1.2734\n",
            "\n",
            "Epoch 8 Loss: 1.2506\n",
            "Time taken for 1 epoch 835.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1805\n",
            "Epoch 9 Batch 50 Loss 1.2002\n",
            "Epoch 9 Batch 100 Loss 1.2271\n",
            "Epoch 9 Batch 150 Loss 1.2344\n",
            "\n",
            "Epoch 9 Loss: 1.2108\n",
            "Time taken for 1 epoch 861.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1687\n",
            "Epoch 10 Batch 50 Loss 1.1702\n",
            "Epoch 10 Batch 100 Loss 1.1811\n",
            "Epoch 10 Batch 150 Loss 1.1484\n",
            "\n",
            "Epoch 10 Loss: 1.1719\n",
            "Time taken for 1 epoch 833.92 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10#memberi nilai iterasi epoch\n",
        "\n",
        "mean = tf.metrics.Mean()#ntuk menghitung rata-rata loss selama pelatihan\n",
        "\n",
        "for epoch in range(EPOCHS):#pelatihan \n",
        "    start = time.time()#menghitung berapa lama satu epoch akan berlangsung.\n",
        "\n",
        "    mean.reset_states()#mereset nilai rata-rata loss di awal setiap epoch untuk menghitung rata-rata loss selama epoch tersebut\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):#oop dalam dataset yang berisi data latihan\n",
        "        logs = model.train_step([inp, target])#melatih model pada batch saat ini. Metode ini mengembalikan nilai loss pada batch tersebut.\n",
        "        mean.update_state(logs['loss'])#memperbarui nilai rata-rata loss dengan loss pada batch saat ini.\n",
        "\n",
        "        if batch_n % 50 == 0:#mencetak loss setiap 50 batch untuk memantau perkembangan pelatihan.\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix.format(epoch=epoch))#menyimpan parameter-parameter model dalam bentuk checkpoint\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')#cetak informasi tentang epoch saat ini, termasuk nomor epoch dan rata-rata loss yang dihasilkan selama epoch tersebut\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')#cetak berapa lama waktu yang diperlukan untuk menyelesaikan satu epoch dalam detik.\n",
        "    print(\"_\"*80)#cetak garis pemisah berupa karakter garis bawah underscore\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))##menyimpan parameter-parameter model dalam bentuk checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-QEttpOnywF"
      },
      "source": [
        "## Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjNPXz-Zng1G"
      },
      "source": [
        "**Potongan Kode Praktikum 2 (Penggunaan Fungsionalitas Keras)**\n",
        "\n",
        "1. Penggunaan Fungsionalitas Keras Terintegrasi: Potongan kode pada praktikum 2 menggunakan lebih banyak fungsionalitas bawaan dari TensorFlow/Keras.\n",
        "2. Penggunaan Layer Keras: Model didefinisikan dengan menggunakan lapisan-lapisan Keras seperti Embedding, GRU, dan Dense. Ini membuat penggunaan model lebih mudah karena Keras mengelola sebagian besar operasi.\n",
        "3. Training dengan Fungsi fit(): Model dilatih menggunakan metode fit() yang lebih terintegrasi dan menangani hampir semua aspek pelatihan.\n",
        "4. Penggunaan Callbacks: Potongan kode ini juga menggunakan callbacks dalam proses pelatihan, khususnya ModelCheckpoint untuk menyimpan bobot model setelah setiap epoch.\n",
        "\n",
        "**Potongan Kode Tugas (Model Custom)**\n",
        "\n",
        "1. Model Custom: Potongan kode di Tugas membahas penggunaan model yang dibuat secara manual dengan pendekatan yang lebih langsung.\n",
        "2. Pembuatan Model dan Pelatihan Langsung: Di sini, Anda membuat kelas CustomTraining yang mewarisi MyModel. Anda kemudian mendefinisikan fungsi train_step secara eksplisit untuk melakukan langkah pelatihan (training step) menggunakan tf.GradientTape dan mengimplementasikan bagian pelatihan secara manual.\n",
        "3. Loop Pelatihan Manual: Anda memiliki loop pelatihan eksplisit yang mengatur pelatihan dalam epoch dan batch secara terperinci.\n",
        "4. Penyimpanan Berdasarkan Epoch: Ada perintah untuk menyimpan bobot model setelah selesainya setiap epoch tertentu.\n",
        "\n",
        "**Perbandingan**\n",
        "\n",
        "1. Level Abstraksi: Potongan kode di Tugas memberikan kontrol yang lebih besar dan membutuhkan lebih banyak penulisan kode eksplisit untuk melatih model.\n",
        "2. Kustomisasi dan Kemudahan Penggunaan: Kode di Tugas memberikan tingkat kustomisasi yang lebih besar dan memungkinkan pengguna untuk mengontrol setiap langkah secara spesifik. Di sisi lain, potongan kode Praktikum 2 menggunakan fungsionalitas bawaan TensorFlow/Keras yang memberikan kemudahan penggunaan dengan menyediakan lebih banyak fitur terintegrasi.\n",
        "3. Pemahaman yang Lebih Mendalam: Kode di Tugas cenderung memberikan pemahaman yang lebih dalam bagaimana setiap langkah dalam pelatihan bekerja, sementara kode Praktikum 2 lebih terfokus pada pemakaian yang lebih mudah dan efisien."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
