{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ox-IKJ9Ag4a"
      },
      "source": [
        "#Setup\n",
        "\n",
        "Import TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEJjrye1AUYq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylNE_71DAlO5"
      },
      "source": [
        "# Download Dataset Shakespeare\n",
        "Sesuaikan dengan lokasi data yang Anda punya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSji8fv8Afdd",
        "outputId": "035c2219-04d3-42ae-9c03-089b6d68c433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7j3ZNoSBVXh"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEadThoZAm3I",
        "outputId": "c48fc87c-b677-4397-b56d-5a3cb8f97705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KED738anBrST",
        "outputId": "689a0d6a-55cb-486c-fc32-aeb91aea4a3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_2v7FRTBsk3",
        "outputId": "3a09cc45-100b-489a-a9cb-b6c363b12381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AERJqA7XBuUP",
        "outputId": "5e70d4c8-6b20-4bca-9e1d-d3fcd032aeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z "
          ]
        }
      ],
      "source": [
        "\n",
        "# Print unique characters\n",
        "for char in vocab:\n",
        "    print(char, end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOL_ZPqyBxpJ"
      },
      "source": [
        "#Olah Teks\n",
        "##Vectorize Teks\n",
        "Sebelum training, Anda perlu mengonversi string menjadi representasi numerik. tf.keras.layers.StringLookup dapat mengubah setiap karakter menjadi ID numerik. Caranya adalah teks akan dipecah menjadi token terlebih dahulu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz1KMCP5Bvqb",
        "outputId": "8e677005-00d2-44a4-ff06-763b084ee3ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3yfkY_bB3jh"
      },
      "source": [
        "sekarang buat tf.keras.layers.StringLookup layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mRN7ZHKB1-m"
      },
      "outputs": [],
      "source": [
        "\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziUqW2tgB6vi"
      },
      "source": [
        "perintah diatas mengconvert token menjadi id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP_LuMxBB5Mh",
        "outputId": "a06a6cd8-db55-4c58-9412-13d4832d5622"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDjNCYEfB9_J"
      },
      "source": [
        "Karena tujuan tutorial ini adalah untuk menghasilkan teks, penting juga untuk membalikkan representasi ini. Untuk ini Anda dapat menggunakan kode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75rnpfDjB8h4"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9ZyOyfWGx27"
      },
      "source": [
        "Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ6Pq_KZB_Uz",
        "outputId": "aca8ff44-0812-4891-dc19-1cda98769c3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wksEj8q8G9MY",
        "outputId": "9e1b5277-db8d-474b-f743-857a62d7f1ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LQtbIWfG-O9"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmFP37MhHFsT"
      },
      "source": [
        "#Prediksi\n",
        "Membuat Training Set dan Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmn56mqGG_Di",
        "outputId": "be94c0a5-dc9e-4343-987b-6bcf6f5e3e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SGr01hrHPII"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcE8fcBEHRsx",
        "outputId": "2c3c5b00-6d23-4590-cb95-3df95dedd330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twJQtoK-HTG0"
      },
      "outputs": [],
      "source": [
        "\n",
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DFYu4WSHisN"
      },
      "source": [
        "Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-SGN4x4HUXA",
        "outputId": "5d31de5f-df42-4d9e-a87a-520876fb59b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKJ1WAzHlNj"
      },
      "source": [
        "akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccBoc9zMHjuY",
        "outputId": "386b9a56-66fc-443f-8b0a-3e33bd7896f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etZbs7DBHn87"
      },
      "source": [
        "Untuk pelatihan, Anda memerlukan kumpulan data pasangan (input, label). Dimana input dan label merupakan urutan. Pada setiap langkah waktu, inputnya adalah karakter saat ini dan labelnya adalah karakter berikutnya. Berikut adalah fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH1C6Pg1HmYA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6ZNRkQcHo-B",
        "outputId": "d54e55c9-e987-4105-fc78-43e8ce89cdf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBOt_zpPHqMY"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBfUnKyzHrMi",
        "outputId": "b71b187a-f819-41fa-f36a-945491953d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkV_JOoSH1xj"
      },
      "source": [
        "#Membuat Batch Training\n",
        "Anda menggunakan tf.data untuk membagi teks menjadi sequence yang dapat diatur. Namun sebelum memasukkan data ini ke dalam model, Anda perlu mengacak data dan mengemasnya ke dalam batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU1HwOEMHsE0",
        "outputId": "7291376a-f5ea-4715-c309-2d1b5ccac2bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJd6WK1tH42D"
      },
      "source": [
        "#Membuat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFdiB0sMH3jx"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGi5QQRAH6mz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54I4vDGZIHWQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI_qQeuuIKJz"
      },
      "source": [
        "#Uji Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ShpYRhaIIhb",
        "outputId": "cbbcdf95-47b7-45d8-8d33-5ae8622e7db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9OvE0BwIMRi",
        "outputId": "35c99caf-3dde-45f6-efb1-50d2eee2b012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPyQg3QoITLU"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rMD8C_7IU_7",
        "outputId": "26046d25-e2e5-45e7-fec3-3cc327959c89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([45, 41,  3,  5, 34, 48, 12, 60, 12, 16, 39, 24,  1,  5, 38,  0, 19,\n",
              "       21, 41,  1, 31, 18, 28, 52, 33, 18, 48,  8, 41,  3, 13, 15, 42,  8,\n",
              "       50, 29, 16, 61, 36, 49, 65, 49, 37, 12,  1,  0, 21, 50, 45, 51, 33,\n",
              "        2, 34, 48, 60, 27, 62,  9, 10, 46, 12, 47, 48, 53, 15, 55, 25, 19,\n",
              "       63, 19, 47, 24, 53, 37, 63, 19, 18, 31, 13,  0, 32, 20, 28, 39, 18,\n",
              "        4, 40, 49, 12, 16, 61,  6, 26,  2, 49, 55, 52, 52, 41, 11])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwHMAy3rIV2y",
        "outputId": "1e139b9b-3fc9-4de7-a2d9-1dbd01e669f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'How? thy wife?\\n\\nELBOW:\\nAy, sir; whom, I thank heaven, is an honest woman,--\\n\\nESCALUS:\\nDost thou dete'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"fb!&Ui;u;CZK\\n&Y[UNK]FHb\\nREOmTEi-b!?Bc-kPCvWjzjX;\\n[UNK]HkflT UiuNw.3g;hinBpLFxFhKnXxFER?[UNK]SGOZE$aj;Cv'M jpmmb:\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1EvBjdeIYvc"
      },
      "source": [
        "#Train Model\n",
        "##Tambahan optimizer dan fungsi loss\n",
        "loss function tf.keras.losses.sparse_categorical_crossentropy standar berfungsi dalam kasus ini karena diterapkan di seluruh dimensi terakhir prediksi. Karena model Anda mengembalikan logits, Anda perlu mengatur flag from_logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-F1eYapIXa7"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL922-A5Ia5x",
        "outputId": "9e7a4e8a-cfcb-40bf-828b-b71f956caeeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189564, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noKzdo82Idys"
      },
      "source": [
        "Model yang baru diinisialisasi tidak boleh terlalu yakin dengan dirinya sendiri, semua log keluaran harus memiliki besaran yang sama. Untuk mengonfirmasi hal ini, Anda dapat memeriksa bahwa eksponensial dari loss rata-rata harus kira-kira sama dengan ukuran kosakata. Loss yang jauh lebih tinggi berarti model tersebut yakin akan jawaban yang salah, dan memiliki inisialisasi yang buruk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmNIoC3pIb2q",
        "outputId": "4e0daf24-dbf8-477c-944c-054af69aa39c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.994026"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgYuhXwhJ5rE"
      },
      "source": [
        "Konfigurasikan prosedur pelatihan menggunakan metode tf.keras.Model.compile. Gunakan tf.keras.optimizers.Adam dengan argumen default dan fungsi loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_udKB2DJ3gJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnRE78KZMsA_"
      },
      "source": [
        "#Konfigurasi Checkpoints\n",
        "Gunakan tf.keras.callbacks.ModelCheckpoint untuk memastikan bahwa checkpoint disimpan selama pelatihan:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU9V75KHMq9c"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BJY-DsxMu8W"
      },
      "source": [
        "#Lakukan Proses Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YGIJEgBMt9v"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL-hiZMkMwit",
        "outputId": "9ace06c9-7e39-4324-fe9b-ebcfb4b9ce94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 961s 6s/step - loss: 2.7292\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 933s 5s/step - loss: 1.9922\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 931s 5s/step - loss: 1.7159\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 939s 5s/step - loss: 1.5559\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 958s 6s/step - loss: 1.4560\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 936s 5s/step - loss: 1.3881\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 931s 5s/step - loss: 1.3340\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 931s 5s/step - loss: 1.2894\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 901s 5s/step - loss: 1.2491\n",
            "Epoch 10/10\n",
            "102/172 [================>.............] - ETA: 6:05 - loss: 1.2037"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCEBH8Vt1a1g"
      },
      "source": [
        "#Generate Teks\n",
        "Berikut ini membuat prediksi satu langkah:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMJBY1EdMxth"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YjempKv1c9Z"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d89xI-Y1eAR",
        "outputId": "0a18eadf-e664-4b6d-c8a4-42411f869488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "I prosfer me to yourself with this drowstood.\n",
            "\n",
            "PROSPERO:\n",
            "O Romeo, come, sir, impossish the like tears!\n",
            "Fie to say and charters true, an hria, forething, treason die;\n",
            "And in all shall seep us, have fouler o'erally.\n",
            "Indeed I, as I came hither.\n",
            "\n",
            "Frown:\n",
            "The verather of all the day is but come\n",
            "That time me to my wife in black moder in.\n",
            "Lond cousins, I'll see thee, demins thrirgly of my soul.\n",
            "Make name, restunes, and like at limbre wated with thing;\n",
            "For every chaption still close; to strike\n",
            "manishem'd hy varviciss: stand up on our neck\n",
            "Juhtier, so Such a dealer.\n",
            "\n",
            "ROMEO:\n",
            "That's so to me; to-dress wish unto the Duke of Noble,\n",
            "You might have ever cannot live the Duke of Warwick and inconstants,\n",
            "The latters of all my fatles mine accoddining,\n",
            "Let us a traitor reteem the life,\n",
            "Ere he that e'er meet hows unto thee breath?\n",
            "Be very lout sorry part, I mean, so old youth.\n",
            "\n",
            "INABELLA:\n",
            "Who were how it?\n",
            "\n",
            "Tyird Servingman:\n",
            "A mat his seniently, do you intelligence:\n",
            "Indeed have I revell's appo, that she shal \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.0010271072387695\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF9yBIjG1gul",
        "outputId": "b1bbf1ae-9cec-417a-e08c-c5fb57e25485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWhat. Ere havin, like in thy face back, her\\nknife, a mark, practist;\\nThese rause goed friar, a Margian make of Darging,\\nOwe, nor, whose holy of thy face, would they have,\\nTo came a great service.\\n\\nLEONTES:\\n&thord, strike:\\nI hold my pity and about me:\\nKnow you, Pompey; being blood bring, as an oath;\\nAnd hot as come here by the ground eyed at once;\\nHe hated been longers: go to this fam,\\nWith good seppected were, I am spirit\\nHath all thy births to this perain's death?\\nO heaving life, or else distresting and\\nFirst, hark, wert thou winter should beat me for my face.\\nBut, to my servick, revolts forbid this side to't.\\n\\nISABELLA:\\n\\nSICINIUS:\\nYou twink change on those that begg'd,\\nWhen once of what rescuse, my master blessing,--\\n\\nKING EDWARD IV:\\nIndeed us Clifford, as\\nif with fool as once, and still fright, all consulines\\nAll too; but said here by the sumplish,\\nThat quaities strange.\\n\\nMERCUTIO:\\nGood my son, or with your voices sing to my mistress begual!\\nCourage us the yight.\\n\\nLady Lord:\\nWhat?\\n\"\n",
            " b\"ROMEO:\\nMy Lord of Hereford wilt thou be 'em, and after much\\nTo see the master. For him, condilition,\\nCorderd not boes:\\nI must be deafured.\\nDeath, horsen you shame.\\n\\nClown:\\nIf thy churre can meet on elieve it;\\nSaughter I want above this firrous;\\nOf what you governous canstly father'd;\\nI am comel so much we pleased. You that have received,\\nLike here of Bohemia and sects of the worst\\nthat hath two course quilt comforted for these eyes.\\nWell, I hear all the sea finds deal, being queen,\\nThat is the sensifed veile ruled that death.\\nTunnt confess, and if shame doth not\\ngometh fool to and my lord.\\n\\nKING HENRY VI:\\n\\nVINTEGS:\\nFor methinks gracting out my vengeance,\\nAnd forces be with well and underred Hereford's issue.\\n\\nHORTENSIA:\\nWell, well, he hath reloved: he hear's long land, and\\nI have pock'd thee, for eyes should hence:\\nGive me Montague, do forting to the gods,\\nAnd yet all. By hery dost sworn to our turn!\\nTwenting thou shalt live you have bined by us:\\nThen we show it in the custom, my sofe's\\nrrop\"\n",
            " b\"ROMEO:\\nThese good no poise comes for the rest:\\nAnd beauty thou the other blessing stargs\\nAnd prove to clear these strong drums, and again'd shid!\\nShe's nea to-night; but what will pay for me?\\n\\nDUKE VINCENTIO:\\nThis is die of unaboid! and have I trust me\\nTo him to see them prayers for Gloucester; but good calls,\\nOur friends shall beard about in so despisate.\\n\\nJULIET:\\nHow long but beast it come forbid?\\n\\nMINENIUS:\\nYou will to seek the glasse of all the blame?\\n\\nBIRGOLIA:\\nTo shame self again, lost in your motter:\\nAnd shew doth protest; how shall get all the wrong\\nmade good to have a vargeness. This fellows\\nWithout direction-strong, without me as at lie,\\nForbear, step dolour; look upon thyself:\\nThis time I wandly hidellidues a sea-stif joun,\\nYour quarrel cannot be too lie that it not and\\nHe took suffer that will desire!\\n\\nDUKE VINCENTIO:\\nO,\\n'Tis lovely that will make givers out that blest.\\nBut such a kind of liberty is,\\nThat had to underth the fair undertly--knee there,\\nOr denier their orly seized t\"\n",
            " b\"ROMEO:\\nPut me, makes thy lord, I could not not draw.\\n\\nD Ke pray'd? oriel! O not remembers;\\nAwaked your wife, all our houses!\\nNear, tell me dow a retiry:\\nWho this set of spice? looked in the royal his\\nFurther Edward lord.\\n\\nLEONTES:\\nElbo, blow the sin,\\nSeem send day had to defend thee for you; King Livil,\\nBut so is not honour, that were Either.\\nNow we have turn to jise thee gone,--\\nTo have thy hearts, or die, pale-feeding fatear, the knit erge\\nproclaimshed at the Frinction, if his blood put me\\nDote my weapons and princes\\nWith brakled princely poized, to become of scarbs\\nFor son and replenge, make thee, my father's life!\\n\\nGLOUCESTER:\\nSo have a night-foot, or some sor--\\n\\nDUCHESS OF YORK:\\nFaith, the mind-that seem required his worthy delay,\\nMay show me so inclined would have the sense. I do beseech you.\\n\\nHast tenderly to the frown;\\nLike to as great advanced earth\\nBannellows, and stir asway?\\n\\nClown:\\nHe same you sleepy?\\n\\nPERDITA:\\nPeace, 'twas made demanss;\\nI did full a drift most gilledromal, call \"\n",
            " b\"ROMEO:\\nSpuried the deed is there.--\\nGive me some old Valentio, command more to die?\\n\\nFLORIZEL:\\nNo, I fear you.\\n\\nCORIOLANUS:\\nWhat wife? a mibleh,\\nWhich he had so quernnculy received.\\nWhat perpetal straight,' quoth my entreates.\\nDure threating thou let her prepert nurse, doubt not:\\nWho unsteared, mark, nor London man; 'tis mistress,\\nAnd not let me give sigh lay and seld my part,\\nAnd give her worse praises all about a blame.\\nWelck me, now she say: if thou ne'er since shall not be good for\\nseem'd in this blow, and will sleep in darken-pins\\nIn hopes on hope of unwith tends and harm of\\nand the markness, more general lay the substitution.\\n\\nLord Mayor:\\nTo accomplish the earnest, why, thou darest.\\nThe lately I remember by praying fool:\\nAnd you? say, foult all seized, so fares\\nIn are a cately edgeals, profaned,\\nFrom thence, to be so far of, Many.\\nThere shall she brought thee to any glorious light.\\n\\nANGELO:\\nWhy, why, no doth meet me;\\nThat should I go: or else hath demoved us;\\nAnd the dast not have you \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw6gxAF01kzx"
      },
      "source": [
        "#Ekspor Model Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvrIalgz1nor",
        "outputId": "d8d88bbc-5cf9-4b6e-f9a9-81fcd1d16e6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x783c805485e0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAl8oqye1pL1",
        "outputId": "cc140fe8-2eb3-40c8-f307-615c7835e4ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "My lord, I heard he substrathe than I play'd,\n",
            "As me of transplene is, for all the kigs'd upon me?\n",
            "L\n"
          ]
        }
      ],
      "source": [
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCCoIOnDnlZf"
      },
      "source": [
        "#Tugas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKf3ZkRI1rzo"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zinf-Ff51xBE"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2yOnRjeEHwz"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tixB0gpELgx",
        "outputId": "254d6b48-9caf-4766-968e-4656d57317f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 768s 4s/step - loss: 2.7339\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x783d0a405ba0>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "model.fit(dataset, epochs=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNSCIMaOEM7x",
        "outputId": "d2f1b26c-0685-4bbc-c946-b7bca319467a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1948\n",
            "Epoch 1 Batch 50 Loss 2.0797\n",
            "Epoch 1 Batch 100 Loss 1.9511\n",
            "Epoch 1 Batch 150 Loss 1.8961\n",
            "\n",
            "Epoch 1 Loss: 1.9950\n",
            "Time taken for 1 epoch 801.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7904\n",
            "Epoch 2 Batch 50 Loss 1.7639\n",
            "Epoch 2 Batch 100 Loss 1.7094\n",
            "Epoch 2 Batch 150 Loss 1.6866\n",
            "\n",
            "Epoch 2 Loss: 1.7168\n",
            "Time taken for 1 epoch 801.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5997\n",
            "Epoch 3 Batch 50 Loss 1.6563\n",
            "Epoch 3 Batch 100 Loss 1.5491\n",
            "Epoch 3 Batch 150 Loss 1.5091\n",
            "\n",
            "Epoch 3 Loss: 1.5560\n",
            "Time taken for 1 epoch 801.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4769\n",
            "Epoch 4 Batch 50 Loss 1.5011\n",
            "Epoch 4 Batch 100 Loss 1.4560\n",
            "Epoch 4 Batch 150 Loss 1.4505\n",
            "\n",
            "Epoch 4 Loss: 1.4567\n",
            "Time taken for 1 epoch 765.73 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4196\n",
            "Epoch 5 Batch 50 Loss 1.4426\n",
            "Epoch 5 Batch 100 Loss 1.4138\n",
            "Epoch 5 Batch 150 Loss 1.3750\n",
            "\n",
            "Epoch 5 Loss: 1.3876\n",
            "Time taken for 1 epoch 759.13 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3285\n",
            "Epoch 6 Batch 50 Loss 1.3779\n",
            "Epoch 6 Batch 100 Loss 1.3255\n",
            "Epoch 6 Batch 150 Loss 1.3773\n",
            "\n",
            "Epoch 6 Loss: 1.3336\n",
            "Time taken for 1 epoch 766.80 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2881\n",
            "Epoch 7 Batch 50 Loss 1.2773\n",
            "Epoch 7 Batch 100 Loss 1.2632\n",
            "Epoch 7 Batch 150 Loss 1.3030\n",
            "\n",
            "Epoch 7 Loss: 1.2881\n",
            "Time taken for 1 epoch 801.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2252\n",
            "Epoch 8 Batch 50 Loss 1.2555\n",
            "Epoch 8 Batch 100 Loss 1.2483\n",
            "Epoch 8 Batch 150 Loss 1.2244\n",
            "\n",
            "Epoch 8 Loss: 1.2469\n",
            "Time taken for 1 epoch 801.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1798\n",
            "Epoch 9 Batch 50 Loss 1.2200\n",
            "Epoch 9 Batch 100 Loss 1.2040\n",
            "Epoch 9 Batch 150 Loss 1.2271\n",
            "\n",
            "Epoch 9 Loss: 1.2070\n",
            "Time taken for 1 epoch 801.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1584\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SOAL\n",
        "Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?"
      ],
      "metadata": {
        "id": "9oEth6ZOWKGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#JAWAB\n",
        "- Perbedaan antara kode tugas dan praktikum 2 terletak pada prosedur pelatihan. Pada praktikum 2, pendekatan pelatihan yang digunakan lebih sederhana dan umum, menggunakan metode 'model.fit'. Di sisi lain, kode tugas menggambarkan pendekatan pelatihan yang lebih spesifik dan kompleks, dengan beberapa kustomisasi. Pendekatan ini melibatkan penggunaan metode train_step dalam model turunan untuk mengatur pelatihan pada tingkat batch. Dalam pendekatan ini, langkah-langkah seperti perhitungan loss, gradien, dan pembaruan bobot model dengan apply_gradients dilakukan secara eksplisit. Selain itu, objek tf.metrics.Mean digunakan untuk menghitung rata-rata loss selama pelatihan. Pendekatan ini memberikan kontrol dan fleksibilitas yang lebih besar dalam mengatur pelatihan model."
      ],
      "metadata": {
        "id": "q5SjZM7QWObE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MHgGEPyWvo6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}