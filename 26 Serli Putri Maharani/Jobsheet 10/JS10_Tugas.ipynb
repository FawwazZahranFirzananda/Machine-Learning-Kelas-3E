{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w9n4avARm_mN"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urzp-ryonH2u",
        "outputId": "1e117e2f-3c1a-405f-cecc-7684b289fe76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elSqho_gnPP-",
        "outputId": "a46e8f71-df26-420f-aede-4ac1224f40ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# # Read, then decode for py2 compat.\n",
        "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# # length of text is the number of characters in it\n",
        "# print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0Xw9MP7nQLe",
        "outputId": "67a077df-e42a-4cd1-b0d6-685aec825901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Take a look at the first 250 characters in text\n",
        "# print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0GJyPdmnSsZ",
        "outputId": "37f3ad86-f1bc-4fc0-8d24-0bdee9204b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "# vocab = sorted(set(text))\n",
        "# print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbRDOPYLnU1t",
        "outputId": "9cd0587b-8ee6-40d5-8b56-4beb3d1201d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example_texts = ['abcdefg', 'xyz']\n",
        "# chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "# chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zSHlnQdCnbkY"
      },
      "outputs": [],
      "source": [
        "# ids_from_chars = tf.keras.layers.StringLookup(\n",
        "# vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yCbWUoxnfDn",
        "outputId": "162235c0-9037-40af-d4a8-436b5247dfd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ids = ids_from_chars(chars)\n",
        "# ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ibx0CPJCnjjB"
      },
      "outputs": [],
      "source": [
        "# chars_from_ids = tf.keras.layers.StringLookup(\n",
        "#     vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjr0W4BlnmdO",
        "outputId": "e1a68b24-c0f1-4301-9215-1bf89d091bc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chars = chars_from_ids(ids)\n",
        "# chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICfs9emRnp4J",
        "outputId": "ee79aa6a-dae2-4f96-db94-33fb31904d4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LebC7R0Vnq38"
      },
      "outputs": [],
      "source": [
        "# def text_from_ids(ids):\n",
        "#     return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRKgvBB4nsV5",
        "outputId": "82365899-adcc-4cc1-e242-cac4ff900087"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "# all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1Z5TUoWCn2V0"
      },
      "outputs": [],
      "source": [
        "# ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KKfz4F4n4AN",
        "outputId": "623ca5d2-0b4f-4d29-de72-e60bd672467e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "# for ids in ids_dataset.take(10):\n",
        "#     print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "STXls0Xrn7ot"
      },
      "outputs": [],
      "source": [
        "# seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul27p1-7n_GP",
        "outputId": "e2fb0e85-c894-47e8-e457-52349ac22de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# for seq in sequences.take(1):\n",
        "#   print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MP0Zj8ToBAt",
        "outputId": "3e616881-8ac8-49f5-a614-6ea2bcc48545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "# for seq in sequences.take(5):\n",
        "#     print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Sd00xFknoCeb"
      },
      "outputs": [],
      "source": [
        "# def split_input_target(sequence):\n",
        "#   input_text = sequence[:-1]\n",
        "#   target_text = sequence[1:]\n",
        "#   return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGLGIE2-oNLj",
        "outputId": "3455250d-34ce-486f-ffb5-908a60dbb3cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uh76vs_CoO0K"
      },
      "outputs": [],
      "source": [
        "# dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0TTTeAgoR24",
        "outputId": "d9bd34b9-a874-43cf-e427-0442347a6604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "# for input_example, target_example in dataset.take(1):\n",
        "#   print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "#   print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq6j-8ploXvf",
        "outputId": "dee71a03-4825-497b-aea3-a925fe732b34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Batch size\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "# # Buffer size to shuffle the dataset\n",
        "# # (TF data is designed to work with possibly infinite sequences,\n",
        "# # so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# # it maintains a buffer in which it shuffles elements).\n",
        "# BUFFER_SIZE = 10000\n",
        "\n",
        "# dataset = (\n",
        "#     dataset\n",
        "#     .shuffle(BUFFER_SIZE)\n",
        "#     .batch(BATCH_SIZE, drop_remainder=True)\n",
        "#     .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8uDDdszkobHC"
      },
      "outputs": [],
      "source": [
        "# # Length of the vocabulary in StringLookup Layer\n",
        "# vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# # The embedding dimension\n",
        "# embedding_dim = 256\n",
        "\n",
        "# # Number of RNN units\n",
        "# rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "algaAI7QodJn"
      },
      "outputs": [],
      "source": [
        "# class MyModel(tf.keras.Model):\n",
        "#   def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "#     super().__init__(self)\n",
        "#     self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "#     self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "#                                    return_sequences=True,\n",
        "#                                    return_state=True)\n",
        "#     self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "#   def call(self, inputs, states=None, return_state=False, training=False):\n",
        "#     x = inputs\n",
        "#     x = self.embedding(x, training=training)\n",
        "#     if states is None:\n",
        "#       states = self.gru.get_initial_state(x)\n",
        "#     x, states = self.gru(x, initial_state=states, training=training)\n",
        "#     x = self.dense(x, training=training)\n",
        "\n",
        "#     if return_state:\n",
        "#       return x, states\n",
        "#     else:\n",
        "#       return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "veXPNF62oetg"
      },
      "outputs": [],
      "source": [
        "# model = MyModel(\n",
        "#     vocab_size=vocab_size,\n",
        "#     embedding_dim=embedding_dim,\n",
        "#     rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTQFVwrGogrV",
        "outputId": "41aa88c2-bd2f-4f84-a69b-4054d3cf599d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# for input_example_batch, target_example_batch in dataset.take(1):\n",
        "#     example_batch_predictions = model(input_example_batch)\n",
        "#     print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGEuA3UzoirX",
        "outputId": "58dbdd43-28e1-48b6-be46-b3bdb79d2061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tdh8obcYol6F"
      },
      "outputs": [],
      "source": [
        "# sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "# sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNv4cvjtorDx",
        "outputId": "b95759ab-8e59-4396-9eca-e0e85de977fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([39, 42, 16, 47,  1, 15, 65, 44,  2, 60, 50, 52, 38, 43, 36, 38, 50,\n",
              "       14, 28, 11, 24, 50, 13, 42, 40, 45, 49, 59, 39, 48, 41, 33, 24, 34,\n",
              "       40,  8,  6, 54,  9, 13, 35, 63, 28, 34, 57,  3, 17, 63, 37, 38, 55,\n",
              "       65, 24, 26, 30, 37,  3, 46, 37, 35, 50, 54, 52, 40,  0,  7, 57, 39,\n",
              "        5, 25, 53, 36, 38, 51, 17, 58, 13, 59, 13, 32, 11,  1, 45, 24,  7,\n",
              "       34, 45, 42, 22, 45, 24, 11, 54, 51, 58, 10,  1, 46,  9, 58])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ch5TFTeosBS",
        "outputId": "b39d7f19-33e8-4dee-d740-30c2e0dd52a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'us well.\\n\\nGLOUCESTER:\\nWell, your imprisonment shall not be long;\\nMeantime, have patience.\\n\\nCLARENCE:'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"ZcCh\\nBze ukmYdWYkAO:Kk?cafjtZibTKUa-'o.?VxOUr!DxXYpzKMQX!gXVkoma[UNK],rZ&LnWYlDs?t?S:\\nfK,UfcIfK:ols3\\ng.s\"\n"
          ]
        }
      ],
      "source": [
        "# print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "# print()\n",
        "# print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XNycc73cou4Y"
      },
      "outputs": [],
      "source": [
        "# loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KggAsty2oxEH",
        "outputId": "d3d7fe88-4e69-4ab8-f76f-dca575657e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1885767, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "# print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "# print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxLBjhCEozmQ",
        "outputId": "945b133d-61bd-4d07-99e1-a902e172231e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.92889"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CUsMhpRPo1Wg"
      },
      "outputs": [],
      "source": [
        "# model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "K_or6Yg4o3N8"
      },
      "outputs": [],
      "source": [
        "# # Directory where the checkpoints will be saved\n",
        "# checkpoint_dir = './training_checkpoints'\n",
        "# # Name of the checkpoint files\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=checkpoint_prefix,\n",
        "#     save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iEYEcFnLo4rw"
      },
      "outputs": [],
      "source": [
        "# EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRbBm-fho7Gx",
        "outputId": "0e541ef5-60a9-42c8-e02c-879b2fdc4044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 13s 51ms/step - loss: 2.7037\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 50ms/step - loss: 1.9760\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 10s 50ms/step - loss: 1.6954\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 1.5365\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.4391\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.3732\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.3208\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.2764\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.2357\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.1949\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.1544\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 1.1126\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.0695\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.0222\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.9725\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.9225\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.8694\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 0.8167\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 0.7673\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.7195\n"
          ]
        }
      ],
      "source": [
        "# history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hi0AOdURo92L"
      },
      "outputs": [],
      "source": [
        "# class OneStep(tf.keras.Model):\n",
        "#   def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "#     super().__init__()\n",
        "#     self.temperature = temperature\n",
        "#     self.model = model\n",
        "#     self.chars_from_ids = chars_from_ids\n",
        "#     self.ids_from_chars = ids_from_chars\n",
        "\n",
        "#     # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "#     skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "#     sparse_mask = tf.SparseTensor(\n",
        "#         # Put a -inf at each bad index.\n",
        "#         values=[-float('inf')]*len(skip_ids),\n",
        "#         indices=skip_ids,\n",
        "#         # Match the shape to the vocabulary\n",
        "#         dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "#     self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "#   @tf.function\n",
        "#   def generate_one_step(self, inputs, states=None):\n",
        "#     # Convert strings to token IDs.\n",
        "#     input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "#     input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "#     # Run the model.\n",
        "#     # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "#     predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "#                                           return_state=True)\n",
        "#     # Only use the last prediction.\n",
        "#     predicted_logits = predicted_logits[:, -1, :]\n",
        "#     predicted_logits = predicted_logits/self.temperature\n",
        "#     # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "#     predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "#     # Sample the output logits to generate token IDs.\n",
        "#     predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "#     predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "#     # Convert from token ids to characters\n",
        "#     predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "#     # Return the characters and model state.\n",
        "#     return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "F63zVEj6pC95"
      },
      "outputs": [],
      "source": [
        "# one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZT4JIFOpF1J",
        "outputId": "f364f3e1-e527-417d-a201-7f964bee2402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "The gates shall have night shall not; since I see them selves\n",
            "My susper reasons frown. Draw your depart,\n",
            "And in thy brother henceforthful steel,\n",
            "Or else i' Gleau-wing-day nor destroy'd;\n",
            "And, with my way, I'ld rather had been thought.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Shou diest in the spleen of fiery drift cause to chide,\n",
            "somewing stund me conserves, could never both\n",
            "The moon, of our solemnits, her grave death opening his;\n",
            "For every captive scarce am fresh accusation, he\n",
            "wife, and that I Geed now, since we his subject's ill-nearing general.\n",
            "The new-muld hazel on person things,\n",
            "For me do wrongfully from my simple pettitoon\n",
            "Lord Angelo that in it all corruption: shall I second\n",
            "What cannot be abrued yet in the body,\n",
            "That there is proud to dry her eyes;\n",
            "Mercy is fled here in the blesser here of Capulet's;\n",
            "Since princes himself, earnest and filf will not trust;\n",
            "For, as I lived, slipp'd in the versel senseen\n",
            "And from secret with us all in Scotland,\n",
            "and the Bunnon Henry for their loss miscall-fountain,\n",
            "God  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1959052085876465\n"
          ]
        }
      ],
      "source": [
        "# start = time.time()\n",
        "# states = None\n",
        "# next_char = tf.constant(['ROMEO:'])\n",
        "# result = [next_char]\n",
        "\n",
        "# for n in range(1000):\n",
        "#   next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "#   result.append(next_char)\n",
        "\n",
        "# result = tf.strings.join(result)\n",
        "# end = time.time()\n",
        "# print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "# print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYTIot-BpHqz",
        "outputId": "9785cd34-2ba1-4ee6-b964-5ce4e423f2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThis is dear preserm that reverse from the room in approved?\\nO, none relust the forest, ready sequile lunation.\\n\\nCLARENCE:\\nHere could Goes not at part cheer: God forbid! What should you know\\nThe tribunes of the Volucas.\\n\\nFLORIZEL:\\nThose are now reason\\nAs shall be long with two kings' holesome men\\nThat wreck of comfort and false will demand.\\n\\nDUKE VINCENTIO:\\nHang, take well Thaight to Naurtuan.\\n\\nThird Citizen:\\nWere you the forest head, your news abroad.\\nDoth he unholds behind, though true--but my love!\\nO, no! how man in your own part, and pray her womb,\\nAnd make her cheek ornaments of soonest disp,\\nresolved that bears the times: old good bloody heart,\\nTo pluck from him to the purpose of one word?\\n\\nTailor:\\nBe not the queen are weather; and,\\nI'll not be shown in absence. Leave me!\\nGo we set on evil hath been a stranger,\\nWith night impude and sestled,--and he be more with more pleasing,\\nDoth much excuse the relaping suffer and the entreat.\\n\\nCAPULET:\\nGo, get a drum: crying all, we stand up\"\n",
            " b\"ROMEO:\\nThat shall be poor in the chample of joy of thy fault,\\nFor I was abjoctment to her house.\\nMy cousin stand upon remorse!\\nBy your good lord attendance that he knaves, he is:\\nBecause his ten Corioli pleased, say\\ntheir triumphs for Craughance but the sun:\\nMore shape your seat, and altouted necessities\\nThat e'er I made by me; and with a poison, which of you\\nNot a dop on it. My pown will serve that I retried\\nNone in the torturer of the face,\\nAnd from her beauty sluiced of your deal quakes, or mild\\nfoundains that he promised here and the king's shore.\\nThat made your grace. My friends stuff in sin\\nHath made him like a burth only and fight.\\nMy pitting in the foe, he is gone and leave\\nHer boy's name be weep: therefore indeed, grant me the\\ncherasiral two first falls of women of constable.\\n\\nTRANIO:\\nMistake the jest of that tender pardon\\nApollow to Seticians.'\\nCalls, pearl and blush in helmes since he were praises,\\nThen his ensoul meass have the appress' all valiant crown?\\nWife! what can I wish th\"\n",
            " b\"ROMEO:\\nGood mother, pardon, I, that thou didst keep to play with her\\nTo meet you under, that falling the bench\\nFast; the better Gloucester's fears, hears his sins,\\nDefactious with a priest; here it draw it\\nTo be my revenue hide false wounds,\\nHow must I see them not. Take up,\\nAnd the nobility of young Prince Edward, till\\nManify the book of bull; but none I'll bear you\\ncontent your grace: where shall we stay, the coron\\nA gentle purpose twenty times of justice,\\nAnd see they curses of death. Unless you have been so sour here.\\n\\nESCALUS:\\nVerily, I warrant him, with thee, Bianca! these lady\\nwithin distatch in presence of the meeting:\\nHalf-day shall they be neck, another foreign sons,\\nThe year gave deed, that means excuped with child.\\n\\nThird Citizen:\\nSee, that had not prove attend out amorout,\\nAnd for the queen over'd.\\n\\nMIRANDA:\\nYes, I get you hence.\\n\\nMARIANA:\\nNoble lady!\\nAn enemio? I wish you, by the root of well;\\nAnd now I am in this pleasant county; but\\nWern me thy husband. Please you\\nTo hear her\"\n",
            " b\"ROMEO:\\nYea, and me, Tranio.\\n\\nTRANIO:\\nGram Rome would you have heard? I perjoin\\nThis foul mild-penury.\\n\\nCORIOLANUS:\\nLet's to be brief.\\n\\nSecond Murderer:\\nI say 'this tenigence, and thou shouldst be talkingly on a\\nfevious off goes by their infancy; he is\\nsever'd indeed, skits and makes made of royal best friend,\\nIn her good seeming, and you be frown'd,\\nHave you am not death: for then? if she comes\\nWith grief and Berk'd, and it your fach in eterna, sails,\\nWhere he hath still should slept be view the air\\nAnd here immose that may be slain,\\nAnd this is non he spour, not right.\\n\\nTRANIO:\\nThen 'twill pray you all have tempted their harmony:\\nyea, a soldier, spack my duty thinks.\\n\\nTRANIO:\\nThat Even here was he that do not so much,\\n'Tis not to think! Camillood of a fool!\\nBetwixion tlandy cannot be must confired.\\nSo stands the first than enemy to the enrmy;\\nRiscand what thou nose'st man?\\nWhere art two manhous witchcraft! Thou dost confess\\nIn forsoon issubout for the common people.\\n\\nCORIOLANUS:\\nTill, perha\"\n",
            " b\"ROMEO:\\nThese honours innocents moon to butcher us.\\n\\nYORK:\\nMy lords, friend Lord Citizen:\\nIt is another formerle this usurping\\nTo fair befal in these love deny in the cleace,\\nOr sing hath none in thee were pays of closet. Office, upon this land\\nAs Peacee to assisting you our general, fond you of your\\nflesh? is it enough, she's corn-dived and cut off,\\nFairil you do his parter of our throne,\\nWith splicks I'ld take from the Duke of Norfolk,\\nThat revell'd grand Greych was the brechest of their seconators.\\n\\nVINCENTIO:\\nLove with me: and thus for the laws langed;\\nWith wishes and honour with the carpet of me\\nAs is the duke, sweet queen! stabled in Peal thou tent?\\nAny price to death! poor prepare\\nTo unstoop'd and his necks, in words, he's vengean, and would tell the power of friends\\nThat, when the last, that cowerfled, make ugsine\\nSo my house: when may stand on account our hobses\\nWhich none or four days bear himself to smare:\\nIf thou hast overforn'd with splitten rejoice.\\nBut come, my lord; take hands\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.716590642929077\n"
          ]
        }
      ],
      "source": [
        "# start = time.time()\n",
        "# states = None\n",
        "# next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "# result = [next_char]\n",
        "\n",
        "# for n in range(1000):\n",
        "#   next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "#   result.append(next_char)\n",
        "\n",
        "# result = tf.strings.join(result)\n",
        "# end = time.time()\n",
        "# print(result, '\\n\\n' + '_'*80)\n",
        "# print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNtj5z4ZpJNW",
        "outputId": "231ed47e-d065-4764-db63-32b03e40a839"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7d1f1ef922c0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "# tf.saved_model.save(one_step_model, 'one_step')\n",
        "# one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuupJAS1pKhH",
        "outputId": "6783dd95-719a-47c0-f17c-cc1633e7e052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Thou hast two at Salisbury, with the recelsor off\n",
            "To see her aches. I have it so vale to do put not\n"
          ]
        }
      ],
      "source": [
        "# states = None\n",
        "# next_char = tf.constant(['ROMEO:'])\n",
        "# result = [next_char]\n",
        "\n",
        "# for n in range(100):\n",
        "#   next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "#   result.append(next_char)\n",
        "\n",
        "# print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzBDwP3Ssuj7"
      },
      "source": [
        "## TUGAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Wt5zczssSQ"
      },
      "source": [
        "Gunakan tf.GradientTape untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca eager execution guide.\n",
        "\n",
        "Prosedurnya adalah :\n",
        "\n",
        "Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "Hitung update dan terapkan pada model dengan optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "A-fkF5MDrk7h"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function # mengubah metode train_step menjadi sebuah graph TensorFlow yang dapat dijalankan secara efisien.\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs # inputs dipisahkan menjadi dua variabel terpisah, yaitu inputs dan labels.\n",
        "    with tf.GradientTape() as tape: # untuk merekam operasi-operasi yang dilakukan pada variabel-variabel yang dibungkus olehnya.\n",
        "      predictions = self(inputs, training=True) # model sedang dalam mode pelatihan, yang berarti beberapa layer mungkin memiliki perilaku yang berbeda saat pelatihan dibandingkan saat pengujian.\n",
        "      loss = self.loss(labels, predictions) # menghitung loss (kerugian) antara labels dan predictions menggunakan metode self.\n",
        "      grads = tape.gradient(loss, model.trainable_variables) # untuk memperbarui nilai-nilai variabel-variabel tersebut selama pelatihan.\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables)) # memperbarui nilai-nilai variabel-variabel dalam model berdasarkan gradien yang dihitung sebelumnya.\n",
        "\n",
        "      return {'loss': loss} # mengembalikan dictionary yang berisi informasi tentang loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_0aqsU6Prtnw"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()), # uatu objek yang memiliki metode get_vocabulary() yang mengembalikan kamus (dictionary) yang memetakan karakter ke bilangan bulat dan len(ids_from_chars.get_vocabulary()) mengambil panjang kamus tersebut, yang mewakili ukuran vokabular model.\n",
        "    embedding_dim=embedding_dim, # untuk menentukan ukuran vektor embedding.\n",
        "    rnn_units=rnn_units) # menentukan jumlah unit dalam layer RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Nnz0b_torukm"
      },
      "outputs": [],
      "source": [
        "# mengompilasi model dengan pengaturan optimizer dan loss function tertentu.\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRWz68e8rzos",
        "outputId": "f1537608-2492-41e3-b130-3a32c76cb7f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 16s 51ms/step - loss: 2.7473\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d1efbc75870>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# melakukan proses pelatihan (training) model menggunakan metode fit.\n",
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-psqXWSr17J",
        "outputId": "e07a1289-1e25-47a2-a280-1f91a51a9f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1796\n",
            "Epoch 1 Batch 50 Loss 2.0584\n",
            "Epoch 1 Batch 100 Loss 1.9787\n",
            "Epoch 1 Batch 150 Loss 1.9084\n",
            "\n",
            "Epoch 1 Loss: 2.0129\n",
            "Time taken for 1 epoch 11.35 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8707\n",
            "Epoch 2 Batch 50 Loss 1.7391\n",
            "Epoch 2 Batch 100 Loss 1.7098\n",
            "Epoch 2 Batch 150 Loss 1.6576\n",
            "\n",
            "Epoch 2 Loss: 1.7406\n",
            "Time taken for 1 epoch 9.98 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6305\n",
            "Epoch 3 Batch 50 Loss 1.5788\n",
            "Epoch 3 Batch 100 Loss 1.5261\n",
            "\n",
            "Epoch 3 Loss: 1.5758\n",
            "Time taken for 1 epoch 10.23 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4695\n",
            "Epoch 4 Batch 50 Loss 1.4856\n",
            "Epoch 4 Batch 100 Loss 1.5375\n",
            "Epoch 4 Batch 150 Loss 1.4651\n",
            "\n",
            "Epoch 4 Loss: 1.4712\n",
            "Time taken for 1 epoch 11.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4001\n",
            "Epoch 5 Batch 50 Loss 1.4140\n",
            "Epoch 5 Batch 100 Loss 1.3995\n",
            "Epoch 5 Batch 150 Loss 1.3898\n",
            "\n",
            "Epoch 5 Loss: 1.3998\n",
            "Time taken for 1 epoch 10.89 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3668\n",
            "Epoch 6 Batch 50 Loss 1.3195\n",
            "Epoch 6 Batch 100 Loss 1.3545\n",
            "Epoch 6 Batch 150 Loss 1.3637\n",
            "\n",
            "Epoch 6 Loss: 1.3453\n",
            "Time taken for 1 epoch 10.35 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.3154\n",
            "Epoch 7 Batch 50 Loss 1.2614\n",
            "Epoch 7 Batch 100 Loss 1.2928\n",
            "Epoch 7 Batch 150 Loss 1.3125\n",
            "\n",
            "Epoch 7 Loss: 1.3003\n",
            "Time taken for 1 epoch 10.32 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2303\n",
            "Epoch 8 Batch 50 Loss 1.2602\n",
            "Epoch 8 Batch 100 Loss 1.2820\n",
            "Epoch 8 Batch 150 Loss 1.2913\n",
            "\n",
            "Epoch 8 Loss: 1.2603\n",
            "Time taken for 1 epoch 10.28 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2367\n",
            "Epoch 9 Batch 50 Loss 1.2521\n",
            "Epoch 9 Batch 100 Loss 1.2273\n",
            "Epoch 9 Batch 150 Loss 1.2411\n",
            "\n",
            "Epoch 9 Loss: 1.2213\n",
            "Time taken for 1 epoch 10.32 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1551\n",
            "Epoch 10 Batch 50 Loss 1.1787\n",
            "Epoch 10 Batch 100 Loss 1.2005\n",
            "Epoch 10 Batch 150 Loss 1.1852\n",
            "\n",
            "Epoch 10 Loss: 1.1835\n",
            "Time taken for 1 epoch 10.41 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10 # menentukan berapa kali model akan melalui seluruh dataset pelatihan selama pelatihan.\n",
        "\n",
        "mean = tf.metrics.Mean() # untuk menghitung rata-rata loss selama satu epoch pelatihan.\n",
        "\n",
        "for epoch in range(EPOCHS): #  loop utama yang akan berjalan sebanyak jumlah epochs yang ditentukan.\n",
        "  start = time.time() # Waktu saat ini dicatat sebelum memulai epoch pelatihan.\n",
        "\n",
        "  mean.reset_states() # untuk menghitung rata-rata loss pada setiap epoch secara terpisah.\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset): # loop yang meloopi melalui setiap batch dalam dataset pelatihan.\n",
        "    logs = model.train_step([inp, target]) # menghitung loss dan melakukan pembaruan bobot dan bias model.\n",
        "    mean.update_state(logs['loss']) #  Loss pada batch tersebut diupdate ke dalam objek mean untuk menghitung rata-rata loss.\n",
        "\n",
        "    if batch_n % 50 == 0: # setiap batch yang kelipatan 50, template string template dibuat untuk mencetak informasi epoch, batch, dan loss pada batch tersebut.\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch)) # Model disimpan (checkpoint) setiap 5 epochs menggunakan metode save_weights.\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDiM8i-asyy4"
      },
      "source": [
        "## SOAL TUGAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aLL_NQWs3D6"
      },
      "source": [
        "Jelaskan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
        "\n",
        "=> Perbedaan antara kode tugas dan praktikum 2 terletak pada pendekatan pelatihan yang digunakan. Pada praktikum 2, digunakan pendekatan pelatihan sederhana menggunakan 'model.fit'. Namun, dalam kode tugas, diterapkan pendekatan pelatihan yang lebih khusus dan kompleks. Dalam pendekatan khusus ini, didefinisikan metode 'train_step' dalam turunan model, yang mengontrol pelatihan pada tingkat batch. Metode ini secara eksplisit menghitung nilai loss, gradien, dan dilakukan pembaruan bobot model. Selain itu, digunakan objek 'tf.metrics.Mean' untuk menghitung rata-rata loss selama pelatihan. Jadi bisa disimpulkan kalau pendekatan ini memberi banyak kontrol dan fleksibilitas yang tinggi dalam pelatihan model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
